{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac156cf",
   "metadata": {},
   "source": [
    "## NLP_Assignment_1\n",
    "1. Explain One-Hot Encoding\n",
    "2. Explain Bag of Words\n",
    "3. Explain Bag of N-Grams\n",
    "4. Explain TF-IDF\n",
    "5. What is OOV problem?\n",
    "6. What are word embeddings?\n",
    "7. Explain Continuous bag of words (CBOW)\n",
    "8. Explain SkipGram\n",
    "9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb54131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0           0            0          1\n",
      "1           0            1          0\n",
      "2           1            0          0\n"
     ]
    }
   ],
   "source": [
    "'''Ans 1:- One-Hot Encoding is a technique used to represent\n",
    "categorical data as binary vectors. Each category is assigned a unique\n",
    "binary value, with only one bit set to 1 and the rest as 0s. For\n",
    "example, in Python This code converts the 'Color' column into\n",
    "one-hot encoded vectors, with a binary representation for each\n",
    "color category.'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Color': ['Red', 'Green', 'Blue']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(df, columns=['Color'])\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0853154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 2:- Bag of Words (BoW) is a text representation method where\n",
    "the frequency of each word in a document is counted,\n",
    "disregarding word order. It creates a sparse vector of word frequencies\n",
    "for each document. In Python This code converts a list of text\n",
    "documents into a BoW representation using scikit-learn's\n",
    "CountVectorizer.\n",
    "\n",
    "Each row corresponds to a document, and each column\n",
    "corresponds to a unique word in the corpus.This BoW matrix allows us\n",
    "to represent text data in a numerical format suitable for\n",
    "machine learning algorithms.  \n",
    "\n",
    "like:-\n",
    "The word \"this\" appears 1 time.\n",
    "The word \"is\" appears 1 time.\n",
    "The word \"the\" appears 1 time.\n",
    "The word \"first\" appears 1 time.\n",
    "The word \"document\" appears 1 time.\n",
    "The word \"second\" appears 0 times.\n",
    "The word \"and\" appears 0 times.\n",
    "The word \"third\" appears 1 time.\n",
    "The word \"one\" appears 0 times.'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\",\n",
    "          \"This document is the second document.\",\n",
    "          \"And this is the third one.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0fa407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'is fun' 'learning' 'learning is' 'love' 'love machine'\n",
      " 'machine' 'machine learning']\n",
      "[[0 0 0 1 0 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 3:- Bag of N-Grams is an extension of the Bag of Words (BoW)\n",
    "model that considers sequences of N consecutive words (N-grams)\n",
    "in text data. It captures not only individual words but also\n",
    "word combinations, providing better context. For example, in\n",
    "the sentence \"I love machine learning,\" the 2-grams would be\n",
    "\"I love\" and \"love machine,\" giving more insight than BoW.In\n",
    "this code, ngram_range=(1, 2) specifies 1-grams (individual\n",
    "words) and 2-grams (word pairs). The resulting BoW matrix\n",
    "includes both single words and word pairs as features.\n",
    "\n",
    "This Bag of N-Grams representation captures both\n",
    "individual words and meaningful word combinations, providing richer\n",
    "information than a simple Bag of Words model.'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"I love machine learning\", \"Machine learning is fun\"]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05dbd7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 4:- TF-IDF (Term Frequency-Inverse Document Frequency) is a\n",
    "numerical statistic used to evaluate the importance of a term in a\n",
    "document within a corpus. It considers both the frequency of a term\n",
    "in a document (TF) and how unique it is across the entire\n",
    "corpus (IDF). Higher TF-IDF scores indicate more important\n",
    "terms.This code calculates TF-IDF values for words in the given text\n",
    "documents, representing the importance of each term.'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\",\n",
    "          \"This document is the second document.\",\n",
    "          \"And this is the third one.\"]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc715b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV>his is a sentence with an OOV word.\n"
     ]
    }
   ],
   "source": [
    "'''Ans 5:- The out-of-vocabulary (OOV) problem is a problem in\n",
    "natural language processing (NLP) where a word in the input text\n",
    "is not present in the vocabulary of the model. This can\n",
    "happen when the model is trained on a limited dataset or when the\n",
    "input text contains new or rare words.In this example, the\n",
    "handle_oov_words() function replaces any word in the text that is not in the\n",
    "vocabulary with the placeholder token <OOV>. This allows the model to\n",
    "continue processing the text even if it encounters words that it\n",
    "does not know.  The OOV problem can be a challenge for NLP\n",
    "models, but there are a number of techniques that can be used to\n",
    "handle it.\n",
    "\n",
    "1. Providing a larger vocabulary: This can be done by training the model\n",
    "   on a larger dataset or by adding new words to the vocabulary manually.\n",
    "2. Using a statistical language model: This can help the model to predict \n",
    "   the meaning of OOV words based on the context in which they appear.\n",
    "3. Using a neural machine translation model: This can learn to translate \n",
    "   OOV words from one language to another.'''\n",
    "\n",
    "def handle_oov_words(text):\n",
    "    for word in text:\n",
    "        if word not in vocabulary:\n",
    "            text = text.replace(word, \"<OOV>\")\n",
    "        return text\n",
    "\n",
    "text = \"This is a sentence with an OOV word.\"\n",
    "\n",
    "# Replace OOV words with <OOV>\n",
    "text = handle_oov_words(text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ebf523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 6:- Word embeddings are numerical representations of words in\n",
    "a way that captures semantic relationships. They map words\n",
    "to dense vectors in a continuous vector space. Word2Vec is a\n",
    "popular technique to create word embeddings.This code uses\n",
    "Word2Vec to create word embeddings and retrieve the vector\n",
    "representation for the word \"NLP.\"\n",
    "\n",
    "The output we get is a 100-dimensional word embedding\n",
    "vector for the word \"NLP\" generated by Word2Vec. Each dimension\n",
    "in the vector represents a different aspect of the word's\n",
    "meaning, learned from the context in which it appears in the\n",
    "training data. These embeddings are used to capture semantic\n",
    "similarities between words and are widely used in natural language\n",
    "processing tasks.'''\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [['I', 'love', 'NLP'], ['Word', 'embeddings', 'are', 'useful']]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "vector = model.wv['NLP']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e21dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 7:- Continuous Bag of Words (CBOW) is a word embedding\n",
    "technique in natural language processing. Unlike Word2Vec's\n",
    "Skip-gram, CBOW predicts a target word from its context words. It\n",
    "aims to learn word representations by minimizing the prediction\n",
    "error. CBOW is efficient and performs well on smaller datasets.\n",
    "To train a CBOW model using Gensim's Word2Vec, we do need to\n",
    "first build a vocabulary from our training data.'''\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['i', 'love', 'machine', 'learning'],\n",
    "    ['word', 'embeddings', 'are', 'useful'],\n",
    "    ['machine', 'learning', 'is', 'fun'],\n",
    "]\n",
    "\n",
    "# Create and train the CBOW model\n",
    "model = Word2Vec(sentences, vector_size=100, window=2, sg=0, min_count=1, workers=4)\n",
    "\n",
    "# Access the word vectors\n",
    "vector = model.wv['machine']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72be7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 8:- This code uses Gensim's Word2Vec to train a Skip-gram word\n",
    "embedding model on a small dataset. It learns to represent words as\n",
    "dense vectors of size 100 while considering a window of 2 words\n",
    "around each target word. The 'sg' parameter is set to 1 for\n",
    "Skip-gram. Finally, it retrieves the word vector for the word\n",
    "\"learning\" learned from its context in the sentences.'''\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['i', 'love', 'machine', 'learning'],\n",
    "    ['word', 'embeddings', 'are', 'useful'],\n",
    "    ['machine', 'learning', 'is', 'fun'],\n",
    "]\n",
    "\n",
    "# Create and train the Skip-gram model\n",
    "model = Word2Vec(sentences, vector_size=100, window=2, sg=1, min_count=1, workers=4)\n",
    "\n",
    "# Access the word vectors\n",
    "vector = model.wv['learning']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7bef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Ans 9:- GloVe (Global Vectors for Word Representation) is an\n",
    "unsupervised word embedding technique designed to capture the semantic\n",
    "meaning of words in a large corpus of text. It focuses on the\n",
    "co-occurrence statistics of words within a context window to create word\n",
    "vectors.  GloVe constructs a global word-to-word co-occurrence\n",
    "matrix that quantifies how often pairs of words appear together\n",
    "in the same context. It then uses matrix factorization\n",
    "techniques to learn word embeddings. The key insight behind GloVe is\n",
    "that word vectors should reflect the relationships between\n",
    "words accurately, which means words with similar meanings will\n",
    "have similar vectors.  GloVe embeddings have several\n",
    "advantages, including their ability to capture semantic relationships,\n",
    "handle rare words, and generalize well to various NLP tasks. They\n",
    "are pre-trained on extensive text corpora, making them useful\n",
    "for downstream applications like sentiment analysis, machine\n",
    "translation, and named entity recognition. GloVe has become a\n",
    "fundamental tool in natural language processing, facilitating the\n",
    "representation of words in a continuous vector space, enabling richer and\n",
    "more effective text analysis.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
