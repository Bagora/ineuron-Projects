{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5bc7e73",
   "metadata": {},
   "source": [
    "## NLP_Assignment_7\n",
    "1. Explain the architecture of BERT\n",
    "2. Explain Masked Language Modeling (MLM)\n",
    "3. Explain Next Sentence Prediction (NSP)\n",
    "4. What is Matthews evaluation?\n",
    "5. What is Matthews Correlation Coefficient (MCC)?\n",
    "6. Explain Semantic Role Labeling\n",
    "7. Why Fine-tuning a BERT model takes less time than pretraining\n",
    "8. Recognizing Textual Entailment (RTE)\n",
    "9. Explain the decoder stack of  GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 1:- BERT (Bidirectional Encoder Representations from\n",
    "Transformers) is a deep learning model architecture based on the\n",
    "Transformer architecture. It uses a bidirectional context by training\n",
    "on both left and right context words simultaneously. BERT\n",
    "consists of multiple layers of attention-based Transformer\n",
    "encoders. It incorporates two pre-training tasks: Masked Language\n",
    "Model (MLM) and Next Sentence Prediction (NSP). This\n",
    "architecture learns contextualized word representations, making it\n",
    "powerful for various natural language understanding tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 2:- Masked Language Modeling (MLM) is a pre-training task in\n",
    "natural language processing. It's a core component of models like\n",
    "BERT. During MLM, random words in a sentence are masked or\n",
    "replaced with a special token. The model's goal is to predict these\n",
    "masked words based on the context provided by the surrounding\n",
    "words. This bidirectional context learning helps the model\n",
    "capture rich contextual information, resulting in better word\n",
    "embeddings and improving performance on downstream tasks like text\n",
    "classification, question answering, and text generation.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 3:- Next Sentence Prediction (NSP) is a pre-training task in\n",
    "natural language processing, employed in models like BERT. NSP\n",
    "aims to predict whether one sentence in a pair of sentences\n",
    "follows the other in a coherent context. The model learns to\n",
    "differentiate between sentence pairs where the second sentence logically\n",
    "follows the first and those where it does not. NSP helps models\n",
    "understand document-level relationships and improves their ability to\n",
    "generate coherent and contextually accurate text.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ced2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Ans 4:-The Matthews correlation coefficient (MCC) is a metric\n",
    "for evaluating the performance of a binary classifier. It is a\n",
    "more robust metric than accuracy, especially when the classes\n",
    "are imbalanced.  The MCC is calculated as follows:  MCC =\n",
    "(TP*TN - FP*FN) / sqrt((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))\n",
    "where:  TP = True Positives TN = True Negatives FP = False\n",
    "Positives FN = False Negatives'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09af9632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "'''Ans 5:- Matthews Correlation Coefficient (MCC) is a measure used\n",
    "to evaluate the performance of binary classification models.\n",
    "It considers true positives, true negatives, false positives,\n",
    "and false negatives to provide a balanced assessment,\n",
    "especially on imbalanced datasets. MCC ranges from -1 (perfect\n",
    "inverse prediction) to 1 (perfect prediction) with 0 indicating\n",
    "random prediction. It's valuable for assessing classification\n",
    "model quality, especially in situations with uneven class\n",
    "distributions.\n",
    "\n",
    "In this code, we import the matthews_corrcoef function\n",
    "from scikit-learn and calculate the MCC for a set of true and\n",
    "predicted binary labels.'''\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "# ground truth labels and predicted labels\n",
    "true_labels = np.array([1, 0, 1, 1, 0, 0, 1])\n",
    "predicted_labels = np.array([1, 0, 1, 0, 1, 0, 0])\n",
    "\n",
    "# Calculate MCC\n",
    "mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fa542",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 6:- Semantic Role Labeling (SRL) is a natural language\n",
    "processing task that involves identifying and classifying the\n",
    "semantic roles of words or phrases within a sentence. It assigns\n",
    "roles such as \"agent,\" \"patient,\" or \"location\" to words,\n",
    "showing their relationships in a sentence. For example, in the\n",
    "sentence \"John ate the pizza with gusto,\" SRL would label \"John\" as\n",
    "the agent, \"ate\" as the predicate, and \"pizza\" as the patient,\n",
    "revealing the roles and connections between them.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 7:- Fine-tuning a BERT (Bidirectional Encoder Representations\n",
    "from Transformers) model is typically faster than the initial\n",
    "pretraining phase for several reasons:-\n",
    "\n",
    "1. Transfer Learning: Pretraining involves training BERT on a\n",
    "massive corpus of text, which can take a long time. Fine-tuning,\n",
    "on the other hand, starts with pretrained weights, leveraging\n",
    "the knowledge learned during pretraining. This allows\n",
    "fine-tuning to converge faster since the model already has a good\n",
    "understanding of language.\n",
    "\n",
    "2. Smaller Dataset: Fine-tuning is performed on a specific\n",
    "task with a smaller dataset, making it computationally less\n",
    "intensive compared to pretraining, which requires processing vast\n",
    "amounts of text.\n",
    "\n",
    "3. Fewer Training Steps: Fine-tuning typically requires fewer\n",
    "training steps or epochs than pretraining. Since the model's lower\n",
    "layers are mostly frozen during fine-tuning, it doesn't need as\n",
    "many updates.\n",
    "\n",
    "4. Domain-Specific Focus: Fine-tuning tailors the pretrained\n",
    "model to a particular task or domain, allowing it to adapt\n",
    "quickly to the specifics of that task.\n",
    "\n",
    "In summary, fine-tuning is quicker because it builds upon\n",
    "pretrained knowledge and focuses on task-specific adjustments rather\n",
    "than the extensive training needed for pretraining.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea82286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b6bcaf16f7481880cdacc48a62b940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a5f32e9ad6475f97bb2a73fbb469e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cc07ae4658494bb605cef38e7b0561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c187214c5806428d9eaa185ba449b80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hypothesis does not logically follow from the text.\n"
     ]
    }
   ],
   "source": [
    "'''Ans 8:- Recognizing Textual Entailment (RTE) is a natural language\n",
    "processing task where the goal is to determine if a given piece of\n",
    "text (the \"hypothesis\") logically follows from another piece of\n",
    "text (the \"text\" or \"premise\"). It's typically framed as a\n",
    "binary classification problem where the model predicts whether\n",
    "the hypothesis is true or false based on the given premise. \n",
    "\n",
    "In this example, we use a pre-trained BERT model to\n",
    "determine if the hypothesis logically follows from the given text.\n",
    "The model predicts whether the entailment is true or false\n",
    "based on the input.'''\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input text and hypothesis\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "hypothesis = \"A fox jumps over a dog.\"\n",
    "\n",
    "# Tokenize and format input\n",
    "inputs = tokenizer(text, hypothesis, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted class (0 for contradiction, 1 for entailment)\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Interpret the result\n",
    "if predicted_class == 1:\n",
    "    print(\"The hypothesis logically follows from the text.\")\n",
    "else:\n",
    "    print(\"The hypothesis does not logically follow from the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c93ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da76bf3b9e7c4a02aaddafda96c11ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148713b557ff4dc49ab4120fa61af8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4819fc53e76d4f4ca0b16b55a5872403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e2653dc2cc45848fc021837a7bf2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Once upon a time,  I was a little bit of a fan of the original series, but I was also a little bit of a fan of the original series. I was a little bit of a fan of the original series, but I\n"
     ]
    }
   ],
   "source": [
    "'''Ans 9:- The decoder stack in GPT (Generative Pre-trained\n",
    "Transformer) models consists of multiple layers of Transformer\n",
    "decoders. Each decoder layer includes multi-head self-attention\n",
    "mechanisms and feedforward neural networks, similar to the encoder.\n",
    "However, it also incorporates a masked self-attention mechanism to\n",
    "ensure causal generation, where each word is generated based on\n",
    "previous words in an autoregressive manner.'''\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load a pre-trained GPT model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Generate text using the decoder stack\n",
    "input_text = \"Once upon a time, \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text autoregressively\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
