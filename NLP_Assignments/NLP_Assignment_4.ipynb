{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d455e6d",
   "metadata": {},
   "source": [
    "## NLP_Assignment_4\n",
    "1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to- \n",
    "   sequence RNN?\n",
    "2. Why do people use encoderâ€“decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "3. How could you combine a convolutional neural network with an RNN to classify videos?\n",
    "4. What are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?\n",
    "5. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "6. What is a common way to distribute training and execution of a deep RNN across multiple GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e48354",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 1:- Sequence-to-Sequence RNN (Seq2Seq):-\n",
    "Machine Translation: Seq2Seq RNNs are widely used for\n",
    "translating text from one language to another, where the input is a\n",
    "sentence in one language, and the output is the corresponding\n",
    "translation.\n",
    "\n",
    "Speech Recognition: In automatic speech recognition (ASR),\n",
    "Seq2Seq models convert spoken language into written text, where\n",
    "the input is an audio sequence, and the output is the\n",
    "transcribed text.\n",
    "\n",
    "Text Summarization: They can generate concise summaries of\n",
    "long texts, where the input is a document, and the output is a\n",
    "shorter summary.\n",
    "\n",
    "Chatbots: Seq2Seq models enable conversational agents to\n",
    "generate human-like responses by mapping user input sequences to\n",
    "appropriate responses.\n",
    "\n",
    "2. Sequence-to-Vector RNN:-\n",
    "Sentiment Analysis: Sequence data (e.g., a sentence) is\n",
    "transformed into a fixed-length vector representation, which is then\n",
    "used to classify the sentiment of the text.\n",
    "\n",
    "Document Classification: It's used to categorize entire\n",
    "documents or articles into predefined classes based on their\n",
    "content.\n",
    "\n",
    "Named Entity Recognition (NER): NER models can tag\n",
    "sequences of words in a document to identify entities like names,\n",
    "dates, and locations.\n",
    "\n",
    "3. Vector-to-Sequence RNN:-\n",
    "Image Captioning: In this case, a vector representation\n",
    "(e.g., features extracted from an image) is converted into a\n",
    "sequence of words to generate captions or descriptions.\n",
    "\n",
    "Music Generation: Given a vector representation of musical\n",
    "notes or features, an RNN can produce a musical score or\n",
    "sequence of notes.\n",
    "\n",
    "Video Description Generation: It can describe video\n",
    "content by converting video features into a natural language\n",
    "description.\n",
    "\n",
    "Each of these RNN architectures addresses specific tasks\n",
    "by either mapping sequences to sequences, sequences to fixed\n",
    "vectors, or fixed vectors to sequences, making them versatile for\n",
    "various applications in natural language processing, speech\n",
    "recognition, computer vision, and more.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2c0c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 100, 128)             640000    ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 100, 128)             768000    ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 256),                394240    ['embedding_2[0][0]']         \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, 100, 256),           394240    ['embedding_3[0][0]',         \n",
      "                              (None, 256),                           'lstm_2[0][1]',              \n",
      "                              (None, 256)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 100, 6000)            1542000   ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3738480 (14.26 MB)\n",
      "Trainable params: 3738480 (14.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''Ans 2:- Encoder-decoder RNNs are favored over plain\n",
    "sequence-to-sequence RNNs for automatic translation because they effectively\n",
    "handle variable-length input and output sequences. The encoder\n",
    "processes the input sequence, creating a fixed-length context vector\n",
    "that captures the entire input context, which the decoder then\n",
    "uses to generate the output sequence. This design allows for\n",
    "better handling of long and complex sentences during translation.\n",
    "\n",
    "This code defines and prints the summary of a\n",
    "sequence-to-sequence neural machine translation model using Keras and\n",
    "TensorFlow. It consists of an encoder and a decoder, each with an LSTM\n",
    "layer, and is suitable for tasks like language translation. The\n",
    "model.summary() statement displays a summary of the model's architecture\n",
    "and parameter details.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define your hyperparameters and data variables\n",
    "max_input_length = 100\n",
    "input_vocab_size = 5000\n",
    "embedding_dim = 128\n",
    "hidden_units = 256\n",
    "max_output_length = 100\n",
    "output_vocab_size = 6000\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Embedding(input_dim=input_vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units=hidden_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length,))\n",
    "decoder_embedding = Embedding(input_dim=output_vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(units=hidden_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c3c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 30, 64, 64, 3)]      0         []                            \n",
      "                                                                                                  \n",
      " conv3d_1 (Conv3D)           (None, 28, 62, 62, 64)       5248      ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 30, 256)]            0         []                            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 6888448)              0         ['conv3d_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)               (None, 256)                  525312    ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 6888704)              0         ['flatten_1[0][0]',           \n",
      " )                                                                   'lstm_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 10)                   6888705   ['concatenate_1[0][0]']       \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69417610 (264.81 MB)\n",
      "Trainable params: 69417610 (264.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''Ans 3:- To combine a Convolutional Neural Network (CNN) with a\n",
    "Recurrent Neural Network (RNN) for video classification, you can use\n",
    "a two-stream architecture. This approach processes spatial\n",
    "and temporal information separately, improving video\n",
    "understanding. \n",
    "\n",
    "1. Spatial Stream (CNN): Use a 3D CNN to extract spatial features\n",
    "from individual frames in the video.\n",
    "\n",
    "2. Temporal Stream (RNN): Use an RNN (e.g., LSTM) to model the temporal sequence\n",
    "of feature vectors generated by the CNN.\n",
    "\n",
    "3. Fusion: Combine the outputs of the CNN and RNN layers to make a final prediction.\n",
    "\n",
    "This code sets up a two-stream architecture where the CNN\n",
    "processes spatial information, the RNN captures temporal dynamics,\n",
    "and the fused information is used for video classification.\n",
    "Adjust parameters to match your specific task and dataset.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv3D, LSTM, Dense, Flatten, concatenate\n",
    "\n",
    "# Define your hyperparameters and data variables\n",
    "frames = 30  # Number of frames in each video\n",
    "height = 64  # Height of each frame\n",
    "width = 64   # Width of each frame\n",
    "channels = 3  # Number of color channels (e.g., RGB)\n",
    "cnn_output_dim = 256  # Output dimension of the CNN\n",
    "num_classes = 10  # Number of video classes for classification\n",
    "\n",
    "# Spatial Stream (CNN)\n",
    "input_shape = (frames, height, width, channels)\n",
    "cnn_input = Input(shape=input_shape)\n",
    "cnn = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(cnn_input)\n",
    "cnn = Flatten()(cnn)\n",
    "\n",
    "# Temporal Stream (RNN)\n",
    "rnn_input = Input(shape=(frames, cnn_output_dim))\n",
    "rnn = LSTM(units=256)(rnn_input)\n",
    "\n",
    "# Fusion\n",
    "combined = concatenate([cnn, rnn])\n",
    "output = Dense(num_classes, activation='softmax')(combined)\n",
    "\n",
    "model = tf.keras.Model(inputs=[cnn_input, rnn_input], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14e8eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Ans 4:- Using dynamic_rnn() in TensorFlow offers advantages over\n",
    "static_rnn() for dynamic sequence lengths. dynamic_rnn() handles\n",
    "variable-length sequences efficiently during runtime, while static_rnn()\n",
    "requires fixed sequence lengths at graph construction. For\n",
    "instance, in natural language processing, dynamic_rnn() accommodates\n",
    "sentences of varying lengths, improving flexibility and reducing\n",
    "memory usage Here, seq_length provides the actual sequence lengths for\n",
    "variable-length input sequences.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "689579e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Inputs:\n",
      "[[1 2 3 0]\n",
      " [4 5 0 0]\n",
      " [6 7 8 9]]\n",
      "Input Mask:\n",
      "tf.Tensor(\n",
      "[[ True  True  True False]\n",
      " [ True  True False False]\n",
      " [ True  True  True  True]], shape=(3, 4), dtype=bool)\n",
      "Padded Targets:\n",
      "[[10 11 12 13]\n",
      " [14 15  0  0]\n",
      " [16 17 18  0]]\n",
      "Output Mask:\n",
      "tf.Tensor(\n",
      "[[ True  True  True  True]\n",
      " [ True  True False False]\n",
      " [ True  True  True False]], shape=(3, 4), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "'''Ans 5:-  To handle variable-length input sequences and variable-length output\n",
    "sequences in natural language processing tasks like sequence-to-sequence\n",
    "models we use.\n",
    "\n",
    "Variable-Length Input Sequences:-\n",
    "\n",
    "Padding: Pad shorter sequences with a special token (usually zero)\n",
    "to match the length of the longest sequence in the batch. \n",
    "This ensures uniform input dimensions.\n",
    "\n",
    "Masking: Create a binary mask that identifies valid elements in the\n",
    "input sequence and ignores padded tokens during computation. \n",
    "Many deep learning frameworks provide built-in support for masking.\n",
    "\n",
    "Variable-Length Output Sequences:-\n",
    "\n",
    "Padding: Similar to input sequences, pad shorter output sequences to match\n",
    "the length of the longest sequence in the batch. Padding can be applied to both \n",
    "target and predicted sequences. \n",
    "\n",
    "Masking: Create a mask for output sequences to handle variable-length targets.\n",
    "This mask ensures that loss calculations consider only valid positions\n",
    "in the target sequence.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Replace 'input_sequences' and 'target_sequences' with your actual data\n",
    "input_sequences = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8, 9]\n",
    "]\n",
    "\n",
    "target_sequences = [\n",
    "    [10, 11, 12, 13],\n",
    "    [14, 15],\n",
    "    [16, 17, 18]\n",
    "]\n",
    "\n",
    "# Padding and masking for input sequences\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, padding='post')\n",
    "input_mask = tf.math.not_equal(padded_inputs, 0)\n",
    "\n",
    "# Padding and masking for output sequences\n",
    "padded_targets = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, padding='post')\n",
    "output_mask = tf.math.not_equal(padded_targets, 0)\n",
    "\n",
    "# Print the results\n",
    "print(\"Padded Inputs:\")\n",
    "print(padded_inputs)\n",
    "print(\"Input Mask:\")\n",
    "print(input_mask)\n",
    "\n",
    "print(\"Padded Targets:\")\n",
    "print(padded_targets)\n",
    "print(\"Output Mask:\")\n",
    "print(output_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d47cb901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Ans 6:- A common method for distributing training and execution of a deep recurrent\n",
    "neural network (RNN) across multiple GPUs is data parallelism. This approach involves\n",
    "dividing the dataset into batches and assigning each batch to a separate GPU. Key steps\n",
    "include splitting the model, processing data in parallel, aggregating gradients, and\n",
    "updating parameters. Frameworks like TensorFlow and PyTorch offer tools for\n",
    "implementing data parallelism.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
