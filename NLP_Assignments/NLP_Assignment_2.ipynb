{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9277654b",
   "metadata": {},
   "source": [
    "## NLP_Assignment_2\n",
    "1. What are Corpora?\n",
    "2. What are Tokens?\n",
    "3. What are Unigrams, Bigrams, Trigrams?\n",
    "4. How to generate n-grams from text?\n",
    "5. Explain Lemmatization\n",
    "6. Explain Stemming\n",
    "7. Explain Part-of-speech (POS) tagging\n",
    "8. Explain Chunking or shallow parsing\n",
    "9. Explain Noun Phrase (NP) chunking\n",
    "10. Explain Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2f003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the 'news' genre: 100554\n",
      "First 10 words in the 'news' genre: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n"
     ]
    }
   ],
   "source": [
    "'''Ans 1:- Corpora are large collections of written or spoken texts\n",
    "used for linguistic analysis and research. They serve as\n",
    "valuable resources for studying language patterns and trends. For\n",
    "example, the \"Brown Corpus\" contains samples of English text from\n",
    "various sources and genres, enabling researchers to analyze\n",
    "language usage across different contexts and time periods.\n",
    "\n",
    "This Python code uses the NLTK library to access the\n",
    "\"Brown Corpus,\" specifically the \"news\" genre. It then calculates\n",
    "the total word count and displays the first 10 words from that\n",
    "genre'''\n",
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Accessing a specific genre from the Brown Corpus\n",
    "news_text = brown.words(categories='news')\n",
    "\n",
    "# Getting some basic statistics\n",
    "print(f\"Total words in the 'news' genre: {len(news_text)}\")\n",
    "print(f\"First 10 words in the 'news' genre: {news_text[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc2b545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'an', 'important', 'NLP', 'task', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Ans 2:- Tokens are individual units of text, such as words or\n",
    "punctuation marks, separated by spaces or other delimiters. They are\n",
    "the building blocks for natural language processing tasks.\n",
    "Here's a code example in Python using the NLTK library to\n",
    "tokenize a sentence. In this example, the word_tokenize function\n",
    "from NLTK is used to split the sentence into tokens, resulting\n",
    "in a list of words and punctuation marks.'''\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Tokenization is an important NLP task.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004c52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('Natural',), ('language',), ('processing',), ('is',), ('fascinating',), ('.',)]\n",
      "Bigrams: [('Natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'fascinating'), ('fascinating', '.')]\n",
      "Trigrams: [('Natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'fascinating'), ('is', 'fascinating', '.')]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 3:- Unigrams, bigrams, and trigrams are different types of\n",
    "n-grams, which are contiguous sequences of n items (usually words)\n",
    "in a text. Here's a Python code example using NLTK to\n",
    "generate unigrams, bigrams, and trigrams from a sentence.This code\n",
    "tokenizes the input sentence and then uses the ngrams function from\n",
    "NLTK to create unigrams, bigrams, and trigrams from the\n",
    "tokenized words. The output will display the respective n-grams for\n",
    "the given sentence. \n",
    "\n",
    "1. Unigrams: These are single words considered in isolation.\n",
    "2. Bigrams: These are sequences of two consecutive words.\n",
    "3. Trigrams: These are sequences of three consecutive words.'''\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "sentence = \"Natural language processing is fascinating.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Generate unigrams, bigrams, and trigrams\n",
    "unigrams = list(ngrams(tokens, 1))\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d62f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is an', 'is an example', 'an example sentence', 'example sentence for', 'sentence for n-gram', 'for n-gram generation.']\n"
     ]
    }
   ],
   "source": [
    "'''Ans 4:- To generate n-grams from text in Python, we can use the\n",
    "ngrams function from libraries like NLTK or create a custom\n",
    "function. Below is an example of generating n-grams using Python's\n",
    "built-in zip function. The generate_ngrams function takes a text\n",
    "and the desired n as inputs. It splits the text into words. It\n",
    "uses a list comprehension to generate n-grams by zipping\n",
    "together slices of the words list. The result is a list of n-grams,\n",
    "which are joined into strings and returned.'''\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "text = \"This is an example sentence for n-gram generation.\"\n",
    "# we can Change n to generate different n-grams (e.g., 1 for unigrams, 2 for bigrams)\n",
    "n = 3\n",
    "\n",
    "result = generate_ngrams(text, n)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0c1a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\aditya\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Original text: running better and faster\n",
      "Lemmatized text: run well and fast\n"
     ]
    }
   ],
   "source": [
    "'''Ans 5:- Lemmatization is a text normalization technique that\n",
    "reduces words to their base or dictionary form (lemma) while\n",
    "considering word context. It helps maintain linguistic accuracy by\n",
    "transforming words to their root form. For example, \"running\" becomes\n",
    "\"run,\" and \"better\" becomes \"good.\" Here's a Python example using\n",
    "the NLTK library. In this code, the WordNetLemmatizer from\n",
    "NLTK is used to lemmatize the word \"running\" into its base\n",
    "form, \"run,\" considering it as a verb.'''\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Download the English language model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Text to be lemmatized\n",
    "text = \"running better and faster\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Lemmatized text:\", \" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c82a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: jumping\n",
      "Stemmed word: jump\n"
     ]
    }
   ],
   "source": [
    "'''Ans 6:- Stemming is a text normalization technique that reduces\n",
    "words to their root or stem form by removing suffixes. It's a\n",
    "more aggressive approach compared to lemmatization and may\n",
    "produce non-linguistic or partial stems. For example, \"jumping\"\n",
    "becomes \"jump,\" and \"flies\" becomes \"fli.\" Stemming is often used\n",
    "in information retrieval and text analysis for simplifying\n",
    "words to their core form.'''\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word = \"jumping\"\n",
    "stem = stemmer.stem(word)\n",
    "\n",
    "print(f\"Original word: {word}\")\n",
    "print(f\"Stemmed word: {stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6efad6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('play', 'VB'), ('soccer', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Ans 7:- Part-of-speech (POS) tagging is the process of labeling\n",
    "each word in a text with its grammatical category, such as\n",
    "noun, verb, adjective, etc. It helps analyze the syntactic\n",
    "structure and meaning of sentences. In Python, libraries like NLTK\n",
    "and spaCy provide tools for POS tagging.This code tokenizes\n",
    "the text and assigns POS tags to each word, resulting in a\n",
    "list of tuples like ('John', 'NN'), where 'NN' represents a\n",
    "noun.\n",
    "\n",
    "1. 'NNP' stands for Proper Noun (e.g., 'John').\n",
    "2. 'VBZ' stands for Verb, 3rd person singular present (e.g., 'likes').\n",
    "3. 'TO' is the word 'to,' often considered a preposition.\n",
    "4. 'VB' stands for Verb (e.g., 'play').\n",
    "5. 'NN' stands for Noun (e.g., 'soccer').\n",
    "6. '.' represents a punctuation mark (e.g., '.').'''\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = \"John likes to play soccer.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b85615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP The/DT cat/NN) chased/VBD (NP the/DT mouse/NN) ./.)\n"
     ]
    }
   ],
   "source": [
    "'''Ans 8:- Chunking, also known as shallow parsing, is a natural\n",
    "language processing technique that groups words in a sentence into\n",
    "meaningful chunks, such as noun phrases or verb phrases. It aids in\n",
    "identifying higher-level linguistic structures. In Python, libraries\n",
    "like NLTK are used for chunking.This code chunks the input\n",
    "sentence into noun phrases based on a defined grammar, resulting in\n",
    "tree structures that represent the chunks.'''\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "text = \"The cat chased the mouse.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Define a grammar for chunking noun phrases\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "chunks = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53f331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "'''Ans 9:- Noun Phrase (NP) chunking is a technique in natural\n",
    "language processing that identifies and extracts noun phrases from\n",
    "a text. These phrases consist of a noun and its associated\n",
    "words, such as adjectives or determiners. NP chunking helps in\n",
    "extracting meaningful information from sentences.This code identifies\n",
    "and extracts noun phrases from the input sentence, resulting\n",
    "in a tree structure representing the NP chunks.\n",
    "\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Define a grammar for NP chunking\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "chunks = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab9e904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Apple Inc., Label: ORG\n",
      "Entity: Steve Jobs, Label: PERSON\n",
      "Entity: Cupertino, Label: GPE\n",
      "Entity: California, Label: GPE\n",
      "Entity: April 1, 1976, Label: DATE\n"
     ]
    }
   ],
   "source": [
    "'''Ans 10:- Named Entity Recognition (NER) is a natural language\n",
    "processing task that identifies and categorizes named entities in\n",
    "text, such as names of people, places, organizations, and dates.\n",
    "It's used to extract structured information from unstructured\n",
    "text.This code identifies and categorizes named entities in the\n",
    "given text and prints their text and corresponding labels,\n",
    "\n",
    "like:\n",
    "1. \"Apple Inc.\" is identified as an organization (ORG).\n",
    "2. \"Steve Jobs\" is identified as a person (PERSON).\n",
    "3. \"Cupertino\" is identified as a geographical place (GPE).\n",
    "4. \"California\" is identified as a geographical place (GPE).\n",
    "5. \"April 1, 1976\" is identified as a date (DATE).'''\n",
    "\n",
    "import spacy\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(f\"Entity: {entity.text}, Label: {entity.label_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
