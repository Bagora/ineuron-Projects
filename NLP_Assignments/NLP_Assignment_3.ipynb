{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab1435e",
   "metadata": {},
   "source": [
    "## NLP_Assignment_3\n",
    "1. Explain the basic architecture of RNN cell.\n",
    "2. Explain Backpropagation through time (BPTT)\n",
    "3. Explain Vanishing and exploding gradients\n",
    "4. Explain Long short-term memory (LSTM)\n",
    "5. Explain Gated recurrent unit (GRU)\n",
    "6. Explain Peephole LSTM\n",
    "7. Bidirectional RNNs\n",
    "8. Explain the gates of LSTM with equations.\n",
    "9. Explain BiLSTM\n",
    "10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c46c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 10, 32)]             0         []                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOp  (3,)                         0         ['input_4[0][0]']             \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_1[0][0]']\n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.zeros_1 (TFOpLambda)     (None, 128)                  0         ['tf.__operators__.getitem_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " simple_rnn_cell_3 (SimpleR  ((None, 10, 128),            20608     ['input_4[0][0]',             \n",
      " NNCell)                      (None, 10, 128))                       'tf.zeros_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20608 (80.50 KB)\n",
      "Trainable params: 20608 (80.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''Ans 1:- A basic Recurrent Neural Network (RNN) cell consists of an\n",
    "input, a hidden state, and an output. It takes an input and\n",
    "combines it with the previous hidden state to produce an output and\n",
    "update the hidden state. This architecture allows RNNs to capture\n",
    "sequential information.In this example, a SimpleRNN cell is applied\n",
    "to input_data with a sequence length of 10 and an input\n",
    "dimension of 32. The output and hidden state are computed.\n",
    "\n",
    "Layer (type): This column lists the layers in our model.\n",
    "In this case, it includes an input layer (input_4), a lambda\n",
    "layer (tf.compat.v1.shape_1), a slicing layer\n",
    "(tf.__operators__.getitem_1), a lambda layer (tf.zeros_1), and the simple_rnn_cell_3\n",
    "layer representing the RNN cell.  Output Shape: This column\n",
    "specifies the shape of the output of each layer. For example, the\n",
    "RNN cell's output shape is (None, 10, 128), indicating that it\n",
    "produces sequences with 10 time steps and each time step has a\n",
    "128-dimensional output.  Param #: This column displays the number of\n",
    "trainable parameters in each layer. The RNN cell has 20,608\n",
    "trainable parameters.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create an RNN cell\n",
    "rnn_cell = tf.keras.layers.SimpleRNNCell(128)\n",
    "\n",
    "# Input data with shape (batch_size, timesteps, input_dim)\n",
    "input_data = tf.keras.layers.Input(shape=(10, 32))\n",
    "\n",
    "# Initialize the RNN cell with an initial state\n",
    "state = rnn_cell.get_initial_state(input_data)\n",
    "\n",
    "# Apply the RNN cell to input_data\n",
    "output, state_new = rnn_cell(input_data, state)\n",
    "\n",
    "# Create a Keras model\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ef3bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 5ms/step - loss: 0.8691\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4029\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2907\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2505\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2071\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1776\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1713\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1629\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1460\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1349\n",
      "[0.8690770864486694, 0.4029095470905304, 0.29073643684387207, 0.250489741563797, 0.2071378380060196, 0.17755652964115143, 0.17128004133701324, 0.16293483972549438, 0.14598777890205383, 0.1348564475774765]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 2:- Backpropagation Through Time (BPTT) is a training\n",
    "algorithm for recurrent neural networks (RNNs). It computes\n",
    "gradients for weight updates by unfolding the network over a\n",
    "sequence and applying backpropagation. It's used to train RNNs to\n",
    "capture and learn from sequential data.In this code, BPTT is used\n",
    "implicitly when training an RNN model (SimpleRNN) with sequential\n",
    "data.\n",
    "\n",
    "The model was trained for 10 epochs, and the training loss\n",
    "decreased with each epoch, which is a positive sign. The training\n",
    "loss values are recorded in the history.history['loss'] list.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate placeholder training data\n",
    "X_train = np.random.rand(100, 10, 32)\n",
    "y_train = np.random.rand(100, 10)\n",
    "\n",
    "# Define an RNN layer\n",
    "rnn = tf.keras.layers.SimpleRNN(64)\n",
    "\n",
    "# Create a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    rnn,\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model with the placeholder data\n",
    "history = model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Print training loss history\n",
    "print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e8e053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 4ms/step - loss: 0.2572\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2563\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2557\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2552\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2547\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2544\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2540\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2537\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2533\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2531\n",
      "Vanishing Gradients - Training Loss History:\n",
      "[0.25720012187957764, 0.2563176155090332, 0.2556900084018707, 0.2552420496940613, 0.2547486424446106, 0.25436991453170776, 0.2539815604686737, 0.25367727875709534, 0.25330430269241333, 0.25306200981140137]\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 35930088.0000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 35742128.0000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 35563952.0000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 35393660.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 35224872.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 35054092.0000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34890272.0000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 34724108.0000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34561136.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 34400020.0000\n",
      "Exploding Gradients - Training Loss History:\n",
      "[35930088.0, 35742128.0, 35563952.0, 35393660.0, 35224872.0, 35054092.0, 34890272.0, 34724108.0, 34561136.0, 34400020.0]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 3:- Vanishing gradients occur during deep neural network\n",
    "training when gradients become extremely small, causing slow or\n",
    "stalled learning. Conversely, exploding gradients occur when\n",
    "gradients become excessively large, leading to unstable training.\n",
    "These issues hinder convergence. Proper weight initialization,\n",
    "gradient clipping, or activation functions (e.g., ReLU) can\n",
    "mitigate these problems.In the vanishing gradients example, the\n",
    "sigmoid activation function may cause gradients to become small in\n",
    "deep networks. In the exploding gradients example, the weights\n",
    "are initialized with a high standard deviation, causing\n",
    "gradients to become large during training.'''\n",
    "\n",
    "# vanishing gradients\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Generate sample data\n",
    "X_train = np.random.rand(100, 2)\n",
    "y_train = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Print training loss history\n",
    "print(\"Vanishing Gradients - Training Loss History:\")\n",
    "print(history.history['loss'])\n",
    "\n",
    "\n",
    "#  Exploding Gradients\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='linear', kernel_initializer=tf.keras.initializers.RandomNormal(stddev=10.0)),\n",
    "    tf.keras.layers.Dense(10, activation='linear', kernel_initializer=tf.keras.initializers.RandomNormal(stddev=10.0)),\n",
    "    tf.keras.layers.Dense(10, activation='linear', kernel_initializer=tf.keras.initializers.RandomNormal(stddev=10.0))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Generate sample data\n",
    "X_train = np.random.rand(100, 2)\n",
    "y_train = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Print training loss history\n",
    "print(\"Exploding Gradients - Training Loss History:\")\n",
    "print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37150520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 7ms/step - loss: 0.6972 - accuracy: 0.5090\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7044 - accuracy: 0.4930\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6943 - accuracy: 0.5100\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6972 - accuracy: 0.5160\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6937 - accuracy: 0.5140\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6925 - accuracy: 0.5160\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6961 - accuracy: 0.4940\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6904 - accuracy: 0.5250\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6894 - accuracy: 0.5320\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6882 - accuracy: 0.5510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x159c045c7f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Ans 4:- Long Short-Term Memory (LSTM) is a type of recurrent\n",
    "neural network (RNN) architecture designed to capture long-term\n",
    "dependencies in sequential data. It uses specialized memory cells that\n",
    "can store and retrieve information over extended sequences,\n",
    "preventing the vanishing gradient problem. LSTMs are widely used in\n",
    "natural language processing, speech recognition, and time series\n",
    "prediction.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate some sample sequential\n",
    "X_train = np.random.rand(100, 10, 32)\n",
    "\n",
    "# Example target data (binary classification)\n",
    "y_train = np.random.randint(0, 2, size=(100, 10))\n",
    "\n",
    "# Convert the target data to one-hot encoding\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# Create an LSTM-based sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=(10, 32), return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "])\n",
    "\n",
    "# Compile and train the model with the one-hot encoded target data\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818f220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 5:- Gated Recurrent Unit (GRU) is a type of recurrent neural\n",
    "network (RNN) architecture that addresses vanishing gradient\n",
    "issues. It uses two gates, update and reset gates, to control\n",
    "information flow through the network. GRUs are computationally\n",
    "efficient and capture long-term dependencies.'''\n",
    "\n",
    "# using Keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(64, input_shape=(10, 32)),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d316dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 243ms/step\n",
      "Predictions:\n",
      "[[[ 0.26733524 -0.00362986  0.02749137 -0.07468929  0.06776802\n",
      "   -0.09303928 -0.20310137 -0.05415118  0.02489976  0.02617916\n",
      "    0.06073262 -0.01580836 -0.01431295  0.13082503  0.2457199\n",
      "   -0.16151677  0.03091986 -0.00437271 -0.0246149   0.06454744\n",
      "   -0.06482319  0.06943206  0.02438603  0.17618304]\n",
      "  [ 0.25557613  0.00374753  0.00423087 -0.13687383 -0.02286217\n",
      "   -0.14553389 -0.1951163  -0.05624678  0.00701665 -0.00407098\n",
      "    0.11107236 -0.04660718 -0.12474928  0.05307633  0.26353997\n",
      "   -0.24102935 -0.04848466 -0.10405404 -0.14694332  0.17351958\n",
      "   -0.10923171  0.04254302  0.00404397  0.20753352]\n",
      "  [ 0.4286884   0.09452787  0.02479679 -0.09113618 -0.06155593\n",
      "   -0.14676446 -0.18467808 -0.0367738  -0.06626655 -0.0326254\n",
      "    0.13739455 -0.06312048 -0.15425849  0.1423896   0.13180794\n",
      "   -0.2876808  -0.04818841 -0.15206318 -0.2232304   0.14884125\n",
      "   -0.04083747  0.0245479  -0.03587583  0.22523478]\n",
      "  [ 0.43247253  0.11147507  0.08165215 -0.14137278 -0.02024639\n",
      "   -0.1402646  -0.19061272 -0.02417091  0.03231426 -0.09036633\n",
      "    0.13929524 -0.06520948 -0.19089447  0.12852678  0.16638492\n",
      "   -0.28218687 -0.06386986 -0.16998915 -0.25993577  0.19807513\n",
      "   -0.0751916  -0.00729623 -0.10560628  0.22975858]\n",
      "  [ 0.49680063  0.20510426  0.09321146 -0.19917917 -0.00614453\n",
      "   -0.11515525 -0.2498426  -0.05193533  0.11008782 -0.04834779\n",
      "    0.20130274 -0.1162606  -0.2567311   0.18887493  0.2820651\n",
      "   -0.324288   -0.02990413 -0.18353948 -0.20997903  0.26868632\n",
      "   -0.08294062 -0.03895111 -0.0333158   0.25087053]\n",
      "  [ 0.434279    0.20753153  0.04864576 -0.18590613 -0.05075128\n",
      "   -0.14004947 -0.23325549 -0.05336824  0.03897445 -0.01183979\n",
      "    0.09823836 -0.18955068 -0.26182863  0.18530218  0.13322674\n",
      "   -0.3485415  -0.13579458 -0.1849863  -0.39099425  0.37090647\n",
      "   -0.07301475 -0.01299463 -0.02056119  0.26862988]\n",
      "  [ 0.51410115  0.23971617  0.09481999 -0.12955993 -0.05265408\n",
      "   -0.16606066 -0.2573814  -0.0680876   0.03366579  0.02074658\n",
      "    0.14275306 -0.17344345 -0.22815198  0.17119722  0.19122258\n",
      "   -0.2782773  -0.13829412 -0.16081972 -0.3275882   0.2552473\n",
      "   -0.01549586 -0.05854097 -0.12937038  0.31990725]\n",
      "  [ 0.62607455  0.3284569   0.07556674 -0.16104934 -0.1162136\n",
      "   -0.12062418 -0.31682387 -0.0660323  -0.01814749  0.05345438\n",
      "    0.15457395 -0.12416244 -0.31082213  0.1961406   0.2593418\n",
      "   -0.3421027  -0.19472207 -0.1632575  -0.4295475   0.40975797\n",
      "   -0.04000454 -0.01690573 -0.10812964  0.28050196]\n",
      "  [ 0.5877663   0.2698755   0.00789879 -0.2244695  -0.09287374\n",
      "   -0.16987728 -0.28700203  0.00091628 -0.01554296  0.08345253\n",
      "    0.23427978 -0.10989918 -0.3274925   0.18144454  0.25395346\n",
      "   -0.31801525 -0.17092825 -0.14592777 -0.40711528  0.30910763\n",
      "    0.06006758 -0.04732457 -0.07606845  0.3410954 ]\n",
      "  [ 0.61214143  0.3344172   0.02969328 -0.19165306 -0.131571\n",
      "   -0.10343502 -0.3258117  -0.036115    0.02760596  0.08399376\n",
      "    0.27953815 -0.21272059 -0.34730944  0.20350075  0.26516455\n",
      "   -0.33291253 -0.12230907 -0.13300902 -0.36669728  0.35523504\n",
      "    0.05276167 -0.09738064 -0.03943507  0.4031799 ]]]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 6:- We generate a sample input sequence X with shape (1, 10,\n",
    "32) (1 sequence, 10 time steps, and 32 features). We create a\n",
    "peephole LSTM model using tf.keras.layers.LSTM with\n",
    "implementation=2 to enable peephole connections. We use the predict method\n",
    "to make predictions based on the input sequence. Finally, we\n",
    "print the predictions.\n",
    "\n",
    "Peephole Long Short-Term Memory (LSTM) is an extension of\n",
    "the standard LSTM architecture that adds connections from the\n",
    "cell state to the gate units, allowing them to \"peek\" at the\n",
    "cell state. This enables the gate units to consider the\n",
    "long-term memory when making decisions. Example code in Keras to\n",
    "create a peephole LSTM layer\n",
    "\n",
    "The output is a 3D array. It has a Vast use cases\n",
    "across various domains due to its ability to handle\n",
    "sequential data and capture long-term dependencies'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate a sample sequential data\n",
    "\n",
    "# input data with 1 sequence, each with 10 time steps and 32 features\n",
    "X = np.random.rand(1, 10, 32)  \n",
    "\n",
    "# Create a peephole LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(24, input_shape=(10, 32), implementation=2, return_sequences=True)  # Peephole LSTM layer\n",
    "])\n",
    "\n",
    "# Make predictions with the model\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0831bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, 10, 128)           49664     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10, 1)             129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49793 (194.50 KB)\n",
      "Trainable params: 49793 (194.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''Ans 7:- Bidirectional Recurrent Neural Networks (Bi-RNNs) process\n",
    "input sequences in both forward and reverse directions. They\n",
    "capture contextual information from past and future time steps\n",
    "simultaneously. This is useful in natural language processing tasks like\n",
    "machine translation, sentiment analysis, and speech recognition,\n",
    "where understanding the entire context is crucial. Bi-RNNs\n",
    "consist of two hidden layers one for forward processing and one\n",
    "for reverse processing.\n",
    "\n",
    "We use the Bidirectional wrapper to create a Bi-LSTM\n",
    "layer. The input shape is (10, 32), representing a sequence with\n",
    "10 time steps and 32 features. The model is compiled for\n",
    "binary classification using the Adam optimizer and binary\n",
    "cross-entropy loss.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add a Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(10, 32)))\n",
    "\n",
    "# Add an output Dense layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17626d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight 1:\n",
      "[[ 0.1044092   0.13617367  0.03686866 ... -0.02717565 -0.07085549\n",
      "   0.06958772]\n",
      " [-0.12940887  0.0287201   0.02803287 ... -0.09724805  0.12399828\n",
      "   0.08343121]\n",
      " [-0.10845497  0.04670468 -0.1403206  ... -0.00101167 -0.11165921\n",
      "   0.1121667 ]\n",
      " ...\n",
      " [ 0.08135447 -0.044175    0.03935732 ... -0.08649188  0.11829746\n",
      "  -0.00579573]\n",
      " [ 0.0428582   0.07077913 -0.01356767 ... -0.1113455  -0.0474661\n",
      "  -0.12852366]\n",
      " [ 0.03423195  0.12291601 -0.03204381 ...  0.06821437 -0.04079828\n",
      "  -0.04033189]]\n",
      "Weight 2:\n",
      "[[-0.00762558  0.02626179  0.15771492 ...  0.07721288 -0.01652937\n",
      "   0.02090484]\n",
      " [ 0.00017905 -0.05924153  0.04126389 ...  0.01686445  0.00767494\n",
      "  -0.04708205]\n",
      " [ 0.17573795  0.00896977  0.1720978  ... -0.11885785 -0.06003467\n",
      "  -0.07625915]\n",
      " ...\n",
      " [ 0.06276384 -0.03095829 -0.0131504  ... -0.04879331  0.03970512\n",
      "  -0.10233448]\n",
      " [-0.02946273  0.08917596  0.0979659  ...  0.05824479 -0.00486016\n",
      "  -0.00735464]\n",
      " [ 0.0097138   0.02031307  0.10451888 ... -0.03391834  0.0608415\n",
      "  -0.0461176 ]]\n",
      "Weight 3:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 8:- Long Short-Term Memory (LSTM) networks employ gates to\n",
    "manage information flow. The forget gate (f_t) controls what to\n",
    "discard from the cell state, while the input gate (i_t) manages\n",
    "what to add. The candidate cell state (~C_t) proposes an\n",
    "update, and the output gate (o_t) determines the next hidden state\n",
    "(h_t). These equations work together to handle sequential data\n",
    "with long-term dependencies.\n",
    "\n",
    "Forget Gate (f_t) Equation:-\n",
    "f_t = σ(W_f * [h_{t-1}, x_t] + b_f)\n",
    "\n",
    "Input Gate (i_t) Equation:-\n",
    "i_t = σ(W_i * [h_{t-1}, x_t] + b_i)\n",
    "\n",
    "Cell State Update (~C_t) Equation:-\n",
    "~C_t = tanh(W_c * [h_{t-1}, x_t] + b_c)\n",
    "\n",
    "Update Cell State (C_t) Equation:-\n",
    "C_t = f_t * C_{t-1} + i_t * ~C_t\n",
    "\n",
    "Output Gate (o_t) Equation:-\n",
    "o_t = σ(W_o * [h_{t-1}, x_t] + b_o)\n",
    "\n",
    "Final Hidden State (h_t) Equation:-\n",
    "h_t = o_t * tanh(C_t)\n",
    "\n",
    "1. σ represents the sigmoid activation function, which\n",
    "outputs values between 0 and 1.\n",
    "\n",
    "2. tanh represents the hyperbolic tangent activation\n",
    "function, which outputs values between -1 and 1.\n",
    "\n",
    "3. [h_{t-1}, x_t] denotes the concatenation of the previous\n",
    "hidden state (h_{t-1}) and the current input (x_t).\n",
    "\n",
    "4. W_f, b_f, W_i, b_i, W_c, b_c, W_o, and b_o are weight\n",
    "matrices and bias vectors learned during training.\n",
    "\n",
    "We create a simple model with an LSTM layer. We access the\n",
    "LSTM layer using model.layers[0]. We retrieve the weights of\n",
    "the LSTM layer using lstm_layer.get_weights(), which returns a\n",
    "list containing the weights matrices (e.g., kernel weights,\n",
    "recurrent kernel weights, bias vectors). then iterate through the list\n",
    "of weights and print each weight matrix.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# simple model with an LSTM layer\n",
    "model = tf.keras.Sequential([\n",
    "    LSTM(units=64, return_sequences=True, input_shape=(10, 32)),\n",
    "])\n",
    "\n",
    "\n",
    "# Print the LSTM layer's weights\n",
    "lstm_layer = model.layers[0] \n",
    "weights = lstm_layer.get_weights() \n",
    "\n",
    "# Print the weights\n",
    "for i, weight in enumerate(weights):\n",
    "    print(f\"Weight {i + 1}:\")\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d274424",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 9:- Bidirectional Long Short-Term Memory (BiLSTM) is an\n",
    "extension of LSTM that processes input sequences in both forward and\n",
    "reverse directions, capturing context from past and future. This\n",
    "improves understanding of sequence data. Below is a code example\n",
    "using TensorFlow/Keras. This code creates a BiLSTM layer for\n",
    "sequence processing, enhancing its ability to capture context from\n",
    "both directions.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(10, 32))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dfb738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 10:- Bidirectional Gated Recurrent Unit (BiGRU) is an extension\n",
    "of the GRU architecture that processes input sequences in\n",
    "both forward and reverse directions, capturing context from\n",
    "past and future. This improves understanding of sequence data.\n",
    "Below is a code example using TensorFlow/Keras to create a BiGRU\n",
    "layer.This code creates a BiGRU layer for sequence processing,\n",
    "enhancing its ability to capture context from both directions.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, GRU\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Bidirectional(GRU(64, return_sequences=True), input_shape=(10, 32))\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
