{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcaedca",
   "metadata": {},
   "source": [
    "## DL_Assignment_12\n",
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "2. How can we use indexing to do the same operation as unsqueeze?\n",
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "6. Implement matmul using Einstein summation.\n",
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "9. What are the forward pass and backward pass of a neural network?\n",
    "10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bc1cc",
   "metadata": {},
   "source": [
    "### Ans 1\n",
    "\n",
    "`unsqueeze` is a PyTorch method that helps solve certain broadcasting problems by adding a new dimension to a tensor. Broadcasting allows tensors with different shapes to be used in element-wise operations, and adding dimensions using `unsqueeze` can help align the shapes of tensors for broadcasting.\n",
    "\n",
    "In this example, without using `unsqueeze`, the addition operation would throw an error because PyTorch couldn't broadcast the scalar tensor `B` to match the shape of tensor `A`. By adding a new dimension with `B.unsqueeze(0)`, we make the shapes compatible, and broadcasting can proceed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b536dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with shape (3,)\n",
    "A = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create a scalar tensor\n",
    "B = torch.tensor(2)\n",
    "\n",
    "# Attempt to add the scalar tensor to A (broadcasting)\n",
    "result = A + B  # This will throw an error\n",
    "\n",
    "# Use unsqueeze to add a new dimension to B\n",
    "B_expanded = B.unsqueeze(0)  # Shape becomes (1,)\n",
    "result = A + B_expanded     # Broadcasting now works\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3fbb6",
   "metadata": {},
   "source": [
    "### Ans 2\n",
    "\n",
    "You can use indexing to achieve the same operation as `unsqueeze` by explicitly adding a new dimension to a tensor. In PyTorch, you can use slicing with `None` or `np.newaxis` to insert a new dimension.\n",
    "\n",
    "In this code, `B[None]` or `B[np.newaxis]` adds a new dimension to `B`, effectively converting it from a scalar tensor to a tensor with shape `(1,)`. This allows for broadcasting, and you can then perform element-wise operations with tensors of compatible shapes. The result will be `[3, 4, 5]`, as each element of `A` is added to `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52283bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with shape (3,)\n",
    "A = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create a scalar tensor\n",
    "B = torch.tensor(2)\n",
    "\n",
    "# Use indexing to add a new dimension to B\n",
    "B_expanded = B[None]  # You can also use B[np.newaxis]\n",
    "\n",
    "# Perform the element-wise addition with broadcasting\n",
    "result = A + B_expanded\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d2771",
   "metadata": {},
   "source": [
    "### Ans 3\n",
    "\n",
    "You can show the actual contents of the memory used for a tensor in PyTorch by using the `.numpy()` method to convert the tensor to a NumPy array. This allows you to inspect the values stored in the tensor.\n",
    "\n",
    "In this code, `x` is a PyTorch tensor, and `numpy_array` is a NumPy array containing the same data. By printing `numpy_array`, you can see the actual values stored in the tensor.\n",
    "\n",
    "This is useful for debugging and inspecting tensor contents, especially when you want to analyze or manipulate the data within the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1479d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "numpy_array = x.numpy()\n",
    "\n",
    "# Print the NumPy array to inspect its values\n",
    "print(\"NumPy array:\", numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb83ec5",
   "metadata": {},
   "source": [
    "### Ans 4\n",
    "\n",
    "When you add a vector of size 3 to a matrix of size 3x3, the vector is typically added to each row of the matrix. This operation is known as broadcasting in linear algebra and is a common feature in many programming libraries and frameworks for deep learning, such as NumPy, TensorFlow, and PyTorch.\n",
    "\n",
    "Here's how the broadcasting works:\n",
    "\n",
    "1. The vector of size 3 is conceptually expanded to have the same number of rows as the matrix (3x3 matrix, 3 rows).\n",
    "2. Each element of the vector is added to the corresponding element in each row of the matrix.\n",
    "\n",
    "In mathematical notation, if you have a 3x3 matrix A and a 3-element vector B, the operation A + B is equivalent to adding B to each row of A:\n",
    "\n",
    "```\n",
    "A = | a11 a12 a13 |      B = | b1 |\n",
    "    | a21 a22 a23 |          | b2 |\n",
    "    | a31 a32 a33 |          | b3 |\n",
    "\n",
    "A + B = | a11+b1 a12+b2 a13+b3 |\n",
    "        | a21+b1 a22+b2 a23+b3 |\n",
    "        | a31+b1 a32+b2 a33+b3 |\n",
    "```\n",
    "\n",
    "So, in summary, when adding a vector to a matrix, the elements of the vector are added to each row of the matrix.\n",
    "\n",
    "When we run this code, we will see that the vector [10, 20, 30] is added to each row of the 3x3 matrix, resulting in the result matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9b3c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Vector:\n",
      "[10 20 30]\n",
      "\n",
      "Result Matrix after adding the vector to each row:\n",
      "[[11 22 33]\n",
      " [14 25 36]\n",
      " [17 28 39]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 3x3 matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "\n",
    "# Create a 3-element vector\n",
    "vector = np.array([10, 20, 30])\n",
    "\n",
    "# Add the vector to each row of the matrix\n",
    "result = matrix + vector\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "print(\"\\nVector:\")\n",
    "print(vector)\n",
    "\n",
    "print(\"\\nResult Matrix after adding the vector to each row:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ba6ea",
   "metadata": {},
   "source": [
    "### Ans 5\n",
    "\n",
    "Broadcasting and `expand_as` do not necessarily result in increased memory use in terms of creating additional copies of data. These techniques are designed to allow for efficient element-wise operations between arrays or tensors of different shapes without the need to physically replicate data.\n",
    "\n",
    "Here's why:\n",
    "\n",
    "1. **Broadcasting**: Broadcasting is a memory-efficient way to perform operations on arrays or tensors of different shapes by virtually expanding the smaller tensor to match the shape of the larger one, without actually duplicating the data. It is a conceptually useful operation for understanding how the operation would behave, but most modern deep learning frameworks, like NumPy, TensorFlow, and PyTorch, perform these operations efficiently by reusing memory as much as possible. So, while it may appear as though you're performing operations on tensors of the same shape, memory is not significantly increased.\n",
    "\n",
    "2. **`expand_as`**: In PyTorch, `expand_as` is a method that returns a new tensor with the same data but a different shape. It doesn't create a copy of the data but rather provides a different view of the data with a different shape. Therefore, it doesn't result in increased memory use by duplicating the tensor's contents.\n",
    "\n",
    "However, it's important to note that while broadcasting and `expand_as` themselves don't inherently increase memory use, the result of these operations may occupy additional memory if you create a new tensor to store the result. For example, if you perform a broadcasting operation and store the result in a new tensor, that new tensor will consume additional memory. It's essential to be mindful of memory usage when working with large datasets or models.\n",
    "\n",
    "In summary, broadcasting and `expand_as` are memory-efficient techniques for working with tensors of different shapes. They aim to minimize unnecessary memory duplication while allowing for element-wise operations between tensors with different shapes. However, memory usage may increase if you explicitly create new tensors to store the results of these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca62cf5",
   "metadata": {},
   "source": [
    "### Ans 6\n",
    "\n",
    "Einstein summation notation is a concise way to express complex tensor operations. To implement matrix multiplication (matmul) using Einstein summation, we can use the following notation:\n",
    "\n",
    "In this code, we use `np.einsum` with the Einstein summation notation `'ij,jk->ik'` to specify the contraction of indices. Here, 'ij' corresponds to the indices of the first matrix `A`, 'jk' corresponds to the indices of the second matrix `B`, and 'ik' represents the indices of the resulting matrix. The Einstein summation notation succinctly expresses the matrix multiplication operation, and the result is stored in the `result` variable.\n",
    "\n",
    "This implementation simplifies matrix multiplication and is particularly useful when dealing with higher-dimensional tensors and complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06d0038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[0.97399891 0.40197834 0.73907151 0.04095821]\n",
      " [0.03284972 0.86054676 0.82901369 0.51844338]\n",
      " [0.037326   0.27991749 0.3911832  0.99464186]]\n",
      "\n",
      "Matrix B:\n",
      "[[0.61359954 0.51912868 0.65397569 0.2338403  0.29007965]\n",
      " [0.11783376 0.88137234 0.13343758 0.20524628 0.60317475]\n",
      " [0.59564739 0.11767756 0.68782699 0.76209083 0.22447436]\n",
      " [0.42286322 0.21738622 0.11034037 0.14371305 0.86924038]]\n",
      "\n",
      "Result of matmul:\n",
      "[[1.10255764 0.95579924 1.2034833  0.8793906  0.72650558]\n",
      " [0.83458851 0.98577409 0.76373541 0.89059642 1.16533335]\n",
      " [0.70949166 0.52834345 0.44057732 0.50724049 1.13206015]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two matrices A and B\n",
    "A = np.random.rand(3, 4)\n",
    "B = np.random.rand(4, 5)\n",
    "\n",
    "# Perform matmul using Einstein summation\n",
    "result = np.einsum('ij,jk->ik', A, B)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "print(\"\\nResult of matmul:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391473f",
   "metadata": {},
   "source": [
    "### Ans 7\n",
    "\n",
    "In the Einstein summation notation, a repeated index letter on the left-hand side of `einsum` represents a summation (or contraction) over that index. This means that for any repeated index letter, the corresponding dimensions in the input tensors will be multiplied element-wise, and then the results will be summed up.\n",
    "\n",
    "For example, consider the Einstein summation notation `'ij,jk->ik'`. In this notation:\n",
    "\n",
    "- 'ij' indicates that we are performing element-wise multiplication between indices 'i' of the first tensor and 'j' of the second tensor.\n",
    "- The 'j' index is repeated, which means that we sum over all possible values of 'j' after the element-wise multiplication.\n",
    "\n",
    "So, when you use `'ij,jk->ik'` with `einsum`, it performs matrix multiplication. The resulting tensor will have indices 'i' and 'k', and each element in this result is obtained by multiplying corresponding elements in 'i' and 'j' and then summing over all 'j' values.\n",
    "\n",
    "This notation is a concise way to express complex tensor operations, making it easier to understand and implement various mathematical operations involving tensors, such as matrix multiplication, tensor contraction, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff06516",
   "metadata": {},
   "source": [
    "### Ans 8\n",
    "\n",
    "Einstein summation notation is a concise way of expressing complex tensor operations, and it is based on three fundamental rules. These rules are designed to simplify tensor operations and make them more intuitive. Here are the three rules of Einstein summation notation and why they are important:\n",
    "\n",
    "1. **Repetition of Indices:** When an index letter appears twice in a term (once as a subscript and once as a superscript), it implies summation over that index. This rule simplifies expressions by automatically performing the summation operation, reducing the need for explicit summation symbols like the summation symbol (∑). \n",
    "\n",
    "   Example: In the expression `A_{ij} B_{jk}`, the index 'j' is repeated, indicating a summation over 'j'.\n",
    "\n",
    "2. **Matching Indices:** When the same index letter appears in different terms without repetition, it implies multiplication (element-wise multiplication) between the corresponding components of tensors or arrays. This rule allows for straightforward element-wise operations between tensors.\n",
    "\n",
    "   Example: In the expression `A_{ij} B_{ij}`, there is no repetition of indices, so it implies element-wise multiplication between corresponding components of matrices A and B.\n",
    "\n",
    "3. **Free Indices:** Any index letter that appears in the output but not in the input terms is considered a \"free index.\" Free indices are preserved in the result and indicate the shape and dimensions of the output. This rule helps specify the shape of the resulting tensor.\n",
    "\n",
    "   Example: In the expression `A_{ij} B_{jk} -> C_{ik}`, the indices 'i' and 'k' are free indices in the output tensor C, specifying its dimensions.\n",
    "\n",
    "These three rules make Einstein summation notation powerful for expressing tensor operations concisely and intuitively. By following these rules, you can perform a wide range of tensor manipulations with clarity and precision, especially in fields like physics, engineering, and machine learning where tensor operations are prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f74327",
   "metadata": {},
   "source": [
    "### Ans 9\n",
    "\n",
    "The forward pass and backward pass are two fundamental steps in training a neural network, specifically in the context of supervised learning with techniques like backpropagation. These steps are crucial for updating the network's parameters (weights and biases) to minimize the loss function and make predictions more accurate. Here's an overview of both:\n",
    "\n",
    "**1. Forward Pass:**\n",
    "\n",
    "During the forward pass, the input data is fed through the neural network to produce predictions or output. Here's how it typically works:\n",
    "\n",
    "1. **Input Layer:** The input data is passed to the input layer of the neural network.\n",
    "\n",
    "2. **Hidden Layers:** The data is then propagated through one or more hidden layers. Each layer consists of neurons that apply a weighted sum to the inputs, followed by an activation function, to produce an output.\n",
    "\n",
    "3. **Output Layer:** The final hidden layer's output is passed to the output layer, which produces the network's predictions.\n",
    "\n",
    "4. **Prediction:** The neural network's output is compared to the true target values (ground truth) using a loss function that measures the error between predictions and actual targets.\n",
    "\n",
    "The forward pass computes the predicted values and evaluates how well the model is performing with respect to the training data.\n",
    "\n",
    "**2. Backward Pass (Backpropagation):**\n",
    "\n",
    "The backward pass, also known as backpropagation, is where the neural network updates its weights and biases to minimize the loss function. It involves the following steps:\n",
    "\n",
    "1. **Compute Loss Gradient:** Calculate the gradient (derivative) of the loss function with respect to the network's output. This gradient measures how much the loss would change with small adjustments in the output.\n",
    "\n",
    "2. **Backpropagate Error:** Propagate this gradient backward through the network using the chain rule of calculus. Compute the gradients of the loss with respect to the parameters (weights and biases) in each layer.\n",
    "\n",
    "3. **Gradient Descent:** Update the parameters of the network (weights and biases) in the direction that minimizes the loss. This update is typically performed using optimization algorithms like gradient descent or its variants (e.g., Adam, RMSprop).\n",
    "\n",
    "4. **Repeat:** Repeat the forward pass and backward pass for multiple iterations (epochs) over the training data to iteratively improve the model's performance.\n",
    "\n",
    "By repeating the forward and backward passes, the neural network learns to adjust its parameters to make better predictions on the training data. This process continues until the loss converges to a minimum or reaches a satisfactory level.\n",
    "\n",
    "In summary, the forward pass computes predictions and evaluates the model's performance, while the backward pass (backpropagation) updates the model's parameters to minimize the loss, making the predictions more accurate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50eadc",
   "metadata": {},
   "source": [
    "### Ans 10\n",
    "\n",
    "Storing activations calculated for intermediate layers during the forward pass is essential for several reasons in the context of training neural networks:\n",
    "\n",
    "1. **Backpropagation for Gradients:** The primary reason for storing intermediate activations is to use them during the backward pass (backpropagation) to calculate gradients. When computing gradients, you need the intermediate activations to determine how much each parameter (weight and bias) contributed to the error in the final prediction. These gradients are crucial for updating the parameters during training using techniques like gradient descent. Without stored activations, you would need to recompute them during backpropagation, which is inefficient and computationally expensive.\n",
    "\n",
    "2. **Efficient Memory Management:** Neural networks can have many layers and hidden units, and their activations can consume a significant amount of memory. Storing intermediate activations allows you to manage memory efficiently, as you can discard activations that are no longer needed once you've used them for backpropagation. This approach minimizes memory usage while still enabling efficient gradient computation.\n",
    "\n",
    "3. **Vanishing and Exploding Gradients:** In deep neural networks, gradients can suffer from the vanishing gradient or exploding gradient problem, especially in deep architectures. By storing intermediate activations, you can monitor how the gradients evolve through the layers and take measures to address issues like vanishing gradients (e.g., using activation functions like ReLU) or exploding gradients (e.g., gradient clipping) as needed.\n",
    "\n",
    "4. **Debugging and Visualization:** Storing intermediate activations also aids in model debugging and visualization. You can inspect and visualize the activations of different layers to understand how the network transforms the input data at various stages, helping you diagnose issues or gain insights into model behavior.\n",
    "\n",
    "In summary, storing intermediate activations during the forward pass is a crucial part of training neural networks. It enables efficient backpropagation for gradient computation, helps manage memory efficiently, addresses gradient-related issues, and facilitates model debugging and visualization. These benefits collectively contribute to more effective and efficient training of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e521f1",
   "metadata": {},
   "source": [
    "### Ans 11\n",
    "\n",
    "Activations with a standard deviation significantly different from 1 can have several downsides in the context of training neural networks. The standard deviation of activations can affect the convergence and stability of the training process. Here are some of the downsides:\n",
    "\n",
    "1. **Vanishing and Exploding Gradients:** When activations have a very small standard deviation (close to 0), it can lead to vanishing gradients during backpropagation. This means that the gradients of the loss with respect to the model's parameters become extremely small, causing slow convergence or preventing the network from learning effectively. On the other hand, if activations have a very large standard deviation (far from 1), it can lead to exploding gradients, which can make training unstable and result in numerical overflow issues.\n",
    "\n",
    "2. **Saturation of Activation Functions:** Activation functions like sigmoid and tanh saturate when their inputs are too large or too small. This can cause activations to get stuck near the extreme values of the activation function, leading to slow learning. When the standard deviation is too far from 1, it increases the likelihood of activations saturating.\n",
    "\n",
    "3. **Non-uniform Learning Rates:** The learning rate in gradient descent is a critical hyperparameter. When activations have a standard deviation that is far from 1, it can result in non-uniform learning rates for different layers. Layers with large standard deviations may learn too quickly and dominate the training process, while layers with small standard deviations may learn too slowly, making it challenging to find a good balance in learning rates.\n",
    "\n",
    "4. **Loss Landscape Geometry:** The geometry of the loss landscape in deep networks can be affected by the scale of activations. Activations with a non-standard deviation can lead to elongated or stretched loss contours, which can make optimization more challenging and sensitive to hyperparameter choices.\n",
    "\n",
    "5. **Numerical Stability:** Activations that are too large or too small can lead to numerical stability issues during training, especially in deep networks. This can result in NaN (not-a-number) or infinity values, causing the training process to break.\n",
    "\n",
    "To address these downsides, it is common practice to use techniques like batch normalization, weight initialization methods (e.g., He initialization), and gradient clipping to help control the scale of activations and gradients during training. These techniques aim to keep activations and gradients within a reasonable range, close to a standard deviation of 1, to promote stable and efficient training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d817f0",
   "metadata": {},
   "source": [
    "### Ans 12\n",
    "\n",
    "Weight initialization is a crucial technique in training neural networks, and it can help avoid problems related to activations with a standard deviation that is too far from 1, such as vanishing and exploding gradients. Proper weight initialization sets the initial values of the network's weights in a way that promotes stable and efficient training. Here's how weight initialization can help:\n",
    "\n",
    "1. **Avoiding Vanishing and Exploding Gradients:** Weight initialization techniques, like He initialization or Xavier/Glorot initialization, ensure that the weights are initialized with values that are appropriate for the scale of the activations in the network. By carefully selecting the initial weights, you can prevent activations from becoming too small (vanishing gradients) or too large (exploding gradients) during the forward and backward passes.\n",
    "\n",
    "2. **Promoting Consistent Activations:** Weight initialization helps promote consistent activations throughout the network. When the weights are initialized properly, activations in each layer tend to have a similar scale, making it easier for the network to learn and converge. This consistency leads to a more stable and predictable training process.\n",
    "\n",
    "3. **Faster Convergence:** Well-initialized weights can lead to faster convergence during training. When activations have a reasonable scale from the beginning, the network can start learning meaningful representations more quickly, reducing the number of epochs required for training.\n",
    "\n",
    "Here are two commonly used weight initialization techniques:\n",
    "\n",
    "   - **He Initialization (for ReLU-based networks):** He initialization initializes weights with random values drawn from a Gaussian distribution with mean 0 and a standard deviation of sqrt(2 / n), where 'n' is the number of input units to the layer. This initialization is particularly effective for networks that use rectified linear units (ReLU) as activation functions.\n",
    "\n",
    "   - **Xavier/Glorot Initialization (for sigmoid and tanh networks):** Xavier initialization initializes weights with random values drawn from a Gaussian distribution with mean 0 and a standard deviation of sqrt(1 / n), where 'n' is the number of input units to the layer. This initialization is suitable for networks that use activation functions like sigmoid or hyperbolic tangent (tanh).\n",
    "\n",
    "Proper weight initialization, in combination with other techniques like batch normalization and gradient clipping, contributes to more stable and efficient training of deep neural networks by addressing the issues associated with activations and gradients that have extreme scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d6c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
