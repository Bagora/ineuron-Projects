{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6d1bed",
   "metadata": {},
   "source": [
    "## DL_Assignment_3\n",
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "2. Is it OK to initialize the bias terms to 0?\n",
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "6. Name three ways you can produce a sparse model.\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "    \n",
    "    a.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "    \n",
    "    b.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "    \n",
    "    c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "    \n",
    "    d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "    \n",
    "    e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed604b49",
   "metadata": {},
   "source": [
    "## Ans 1\n",
    "\n",
    "Initializing all the weights to the same value, even when using a proper weight initialization technique like He initialization, is generally not recommended. While He initialization is effective in addressing the vanishing/exploding gradient problem, initializing all weights to the same value can still lead to symmetry issues during training.\n",
    "\n",
    "Symmetry issues occur because when all weights have the same value, neurons in the same layer behave identically, causing symmetric weight updates during backpropagation. This symmetry can persist throughout training, limiting the network's capacity to learn diverse features and patterns.\n",
    "\n",
    "He initialization, which initializes weights with small random values according to the number of input units, helps break this symmetry by introducing some diversity in the initial weights. It's a crucial part of weight initialization for deep neural networks.\n",
    "\n",
    "In summary, it's best to use He initialization or similar techniques to initialize weights with small random values, ensuring that each neuron starts with different initial parameters, helping the network learn effectively and preventing symmetry issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286ebcb",
   "metadata": {},
   "source": [
    "### Ans 2\n",
    "\n",
    "Initializing bias terms to 0 is a common practice and often a reasonable choice in many cases when training neural networks. However, it's not the only option, and there are situations where initializing biases differently may be beneficial.\n",
    "\n",
    "Here are some considerations regarding bias initialization:\n",
    "\n",
    "1. **Initialization to 0:** Initializing biases to 0 simplifies the network's initial state, and it's a sensible choice when you want the network to start with no biases and rely solely on the learned weights to make predictions. This can work well in practice, especially with certain activation functions like ReLU.\n",
    "\n",
    "2. **Random Initialization:** Some researchers advocate for initializing biases with small random values, similar to weight initialization, to introduce some diversity from the beginning. This can help break symmetry and might be particularly useful with activation functions that have issues with dead neurons (neurons that always output 0).\n",
    "\n",
    "3. **Learnable Biases:** In some cases, you might want to allow biases to be learned during training. In such cases, you can initialize them to 0 initially, and the network will adjust them as it learns.\n",
    "\n",
    "4. **Domain-Specific Initialization:** Depending on the problem and network architecture, you may choose specialized bias initialization strategies tailored to your specific use case.\n",
    "\n",
    "In summary, while initializing bias terms to 0 is a reasonable default choice, the decision can depend on the specific problem, the choice of activation functions, and whether you want to allow biases to be learned during training. Experimentation and tuning are often required to determine the most effective bias initialization strategy for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131301f",
   "metadata": {},
   "source": [
    "### Ans 3\n",
    "\n",
    "The Scaled Exponential Linear Unit (SELU) activation function offers several advantages over the Rectified Linear Unit (ReLU) activation function:\n",
    "\n",
    "1. **Self-normalization:** One of the most significant advantages of SELU is its ability to self-normalize neural networks. In networks with many layers, activations tend to vanish or explode as they propagate through the layers. SELU helps mitigate this problem, ensuring that activations converge to a mean of 0 and a standard deviation of 1. This leads to more stable gradients and faster training.\n",
    "\n",
    "2. **Continuous and smooth:** Unlike ReLU, SELU is continuous and smooth across its entire domain, including zero. This smoothness can be beneficial in certain situations, such as optimization algorithms that rely on gradients, where ReLU's non-smoothness can introduce challenges.\n",
    "\n",
    "3. **Consistency in deep networks:** SELU maintains its self-normalizing properties even in very deep neural networks. In contrast, traditional activation functions like sigmoid and tanh may not perform well in extremely deep networks due to the vanishing gradient problem. SELU's ability to propagate information effectively through deep networks can lead to improved performance and convergence.\n",
    "\n",
    "Overall, SELU is a powerful activation function that can be especially advantageous when training deep neural networks, offering improved convergence, gradient stability, and consistent performance across different network depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3983a9",
   "metadata": {},
   "source": [
    "### Ans 4\n",
    "\n",
    "The choice of activation function in a neural network depends on the specific characteristics of the problem you are trying to solve and the architecture of your network. Here are common use cases for various activation functions:\n",
    "\n",
    "1. **SELU (Scaled Exponential Linear Unit):**\n",
    "   - Use SELU when building deep neural networks, especially deep feedforward networks or deep recurrent networks.\n",
    "   - It is effective in networks where self-normalization and maintaining consistent gradients are crucial.\n",
    "\n",
    "2. **Leaky ReLU and Its Variants (e.g., Parametric ReLU, Randomized Leaky ReLU):**\n",
    "   - Use Leaky ReLU variants when dealing with the vanishing gradient problem associated with traditional ReLU.\n",
    "   - They are more suitable for deep networks and can handle situations where ReLU might result in dead neurons.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit):**\n",
    "   - Use ReLU as a default choice for most cases, especially in convolutional neural networks (CNNs).\n",
    "   - ReLU is known for its simplicity, computational efficiency, and effectiveness in promoting sparse activations.\n",
    "\n",
    "4. **tanh (Hyperbolic Tangent):**\n",
    "   - Use tanh in scenarios where the output needs to be zero-centered (mean close to 0) and the output range between -1 and 1 is desirable.\n",
    "   - It is commonly used in recurrent neural networks (RNNs) and certain types of autoencoders.\n",
    "\n",
    "5. **Logistic (Sigmoid):**\n",
    "   - Use logistic sigmoid in binary classification problems, where you want the output to be in the range [0, 1] and interpret the result as a probability.\n",
    "   - It is also used in the output layer of multi-class classification problems when combined with softmax.\n",
    "\n",
    "6. **Softmax:**\n",
    "   - Use softmax activation in the output layer for multi-class classification problems, where you want to obtain class probabilities.\n",
    "   - It ensures that the sum of the output values across classes equals 1, making it suitable for classification tasks.\n",
    "\n",
    "Keep in mind that while these are typical use cases, the choice of activation function can also depend on empirical experimentation to find the best-performing function for your specific dataset and problem. Additionally, novel activation functions and variants continue to emerge, and the best choice may evolve with ongoing research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707d96b",
   "metadata": {},
   "source": [
    "### Ans 5\n",
    "\n",
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using Stochastic Gradient Descent (SGD) optimizer can have unintended consequences and lead to issues during training. Momentum is a parameter that controls the influence of past gradients on the current update direction. When set too close to 1, the following problems may arise:\n",
    "\n",
    "1. **Excessive Weight Updates:** High momentum values make the optimizer \"remember\" past gradients for a long time. This can lead to excessively large weight updates, causing the optimization process to overshoot the minimum of the loss function. The model's parameters may oscillate or diverge, preventing convergence.\n",
    "\n",
    "2. **Reduced Learning Rate Effectiveness:** A high momentum effectively reduces the effective learning rate, making the optimizer less responsive to the current gradient information. This can slow down convergence and hinder the optimizer's ability to escape local minima.\n",
    "\n",
    "3. **Instability:** Very high momentum values can introduce instability in training, making it difficult to fine-tune hyperparameters and obtain consistent results.\n",
    "\n",
    "4. **Difficulty in Finding the Optimum:** A momentum value that is too close to 1 can cause the optimizer to become \"stuck\" in a region of parameter space, making it challenging to find the global minimum of the loss function.\n",
    "\n",
    "To avoid these issues, it's typically recommended to use a moderate momentum value in the range of 0.8 to 0.99. The choice of momentum should be made through experimentation and hyperparameter tuning to strike a balance between fast convergence and stability during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6ec85",
   "metadata": {},
   "source": [
    "### Ans 6\n",
    "\n",
    "Producing a sparse model, which has fewer parameters or connections compared to a dense model, is often desirable for reducing computational resources and memory usage. Here are three ways to produce a sparse model:\n",
    "\n",
    "1. **Weight Pruning:**\n",
    "   - Weight pruning involves identifying and removing less important weights from a pre-trained neural network.\n",
    "   - A common technique is magnitude-based pruning, where weights with magnitudes below a certain threshold are set to zero and pruned.\n",
    "   - Structured pruning techniques, such as channel pruning (removing entire channels in convolutional layers) or neuron pruning (removing entire neurons), can also be applied.\n",
    "   - Pruning can significantly reduce the model size with minimal impact on performance, and fine-tuning can be used to recover some lost accuracy.\n",
    "\n",
    "2. **Sparse Activation Functions:**\n",
    "   - Instead of using dense activation functions like ReLU, you can use sparse activation functions like Sparsemax or Gumbel-Softmax.\n",
    "   - These functions encourage sparsity in the activations, resulting in fewer neurons being active during inference.\n",
    "   - Sparse activation functions can be particularly useful in tasks where feature selection or interpretability is important.\n",
    "\n",
    "3. **Knowledge Distillation:**\n",
    "   - Knowledge distillation involves training a smaller, student model (sparse model) to mimic the predictions of a larger, teacher model (dense model).\n",
    "   - The student model learns to approximate the teacher's behavior, effectively inheriting the knowledge of the dense model.\n",
    "   - By transferring knowledge, you can create a smaller model with fewer parameters while maintaining performance close to that of the larger model.\n",
    "\n",
    "Each of these techniques offers a way to produce sparse models, and the choice depends on the specific requirements of your task, available resources, and desired trade-offs between model size and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c859c",
   "metadata": {},
   "source": [
    "### Ans 7\n",
    "\n",
    "Dropout, a regularization technique commonly used in neural networks, can affect training and inference speed differently:\n",
    "\n",
    "1. **Training with Dropout:**\n",
    "   - Dropout is typically applied during training to prevent overfitting. It randomly sets a fraction of neuron activations to zero during each forward and backward pass.\n",
    "   - During training, dropout can slow down the convergence rate because it introduces noise and randomness into the learning process. The network may require more epochs to reach a good solution.\n",
    "   - Training with dropout can also increase the computational time per epoch since it effectively trains multiple subnetworks with different dropped-out neurons.\n",
    "\n",
    "2. **Inference with Dropout:**\n",
    "   - During inference (making predictions on new data), dropout is typically turned off. In this case, dropout does not slow down inference because all neurons are active, and there is no randomness introduced.\n",
    "   - Inference speed with dropout is typically the same as or faster than inference with models that don't use dropout because dropout has no effect during this phase.\n",
    "\n",
    "3. **MC Dropout (Monte Carlo Dropout):**\n",
    "   - MC Dropout is a technique where dropout is applied during both training and inference but with a modification. Instead of using dropout to randomly set neurons to zero, it is used to sample predictions multiple times (e.g., 10 or 100 times) with dropout enabled and then average the results.\n",
    "   - MC Dropout can slow down inference since it requires multiple forward passes with dropout. However, it can provide better uncertainty estimates and improved model calibration, which can be beneficial in certain applications, such as Bayesian deep learning and uncertainty quantification.\n",
    "\n",
    "In summary, dropout can slow down training but doesn't affect inference speed because it's typically turned off during inference. MC Dropout, on the other hand, can slow down inference due to the multiple forward passes but offers improved uncertainty estimates and model reliability. The choice between these techniques depends on the specific requirements of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b28da",
   "metadata": {},
   "source": [
    "### Ans 8\n",
    "\n",
    "Training a deep neural network on the CIFAR-10 dataset with various configurations is a complex task that involves significant computational resources and multiple iterations. I'll provide you with a high-level overview of the steps you can follow for each part of the exercise:\n",
    "\n",
    "a. **Build a DNN with 20 Hidden Layers:**\n",
    "   - Create a deep neural network with 20 hidden layers, each having 100 neurons.\n",
    "   - Use He initialization for weight initialization.\n",
    "   - Apply the ELU activation function.\n",
    "\n",
    "b. **Train the Network with Nadam and Early Stopping:**\n",
    "   - Load the CIFAR-10 dataset using `keras.datasets.cifar10.load_data()`.\n",
    "   - Preprocess the data, normalize pixel values, and one-hot encode class labels.\n",
    "   - Build the DNN with the specified architecture.\n",
    "   - Compile the model using the Nadam optimizer, appropriate loss function (e.g., categorical cross-entropy), and evaluation metric.\n",
    "   - Train the model with early stopping based on validation loss.\n",
    "   - Experiment with learning rates to find the best one for your model.\n",
    "\n",
    "c. **Add Batch Normalization:**\n",
    "   - Modify the architecture to include Batch Normalization layers after each hidden layer.\n",
    "   - Compare the learning curves regarding convergence speed and model performance.\n",
    "\n",
    "d. **Replace Batch Normalization with SELU:**\n",
    "   - Adjust the input data to be standardized (zero mean, unit variance).\n",
    "   - Use LeCun normal initialization (HE initialization with different scaling).\n",
    "   - Ensure that the DNN contains only dense layers with SELU activation.\n",
    "   - Train the model and compare it with previous results.\n",
    "\n",
    "e. **Regularize with Alpha Dropout and Try MC Dropout:**\n",
    "   - Add Alpha Dropout layers for regularization.\n",
    "   - Optionally, apply MC Dropout by sampling predictions multiple times during inference and averaging the results to capture model uncertainty.\n",
    "\n",
    "Remember to fine-tune hyperparameters, such as dropout rates, batch sizes, and the number of epochs, as well as experiment with different learning rates and model architectures to optimize performance.\n",
    "\n",
    "Due to the complexity and resource-intensive nature of this task, it may require significant computation time and experimentation. Consider using cloud-based GPU resources or distributed training if available to expedite the process.\n",
    "\n",
    "Here's example demonstrating the implementation of a deep neural network with various configurations for training on the CIFAR-10 dataset using TensorFlow and Keras. This code demonstrates how to build a deep neural network with various configurations and train it on the CIFAR-10 dataset. We can modify hyperparameters and experiment with different configurations to observe their effects on training and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ab3183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "625/625 - 56s - loss: 2.5801 - accuracy: 0.1023 - val_loss: 2.3063 - val_accuracy: 0.0980 - 56s/epoch - 90ms/step\n",
      "Epoch 2/100\n",
      "625/625 - 21s - loss: 2.3467 - accuracy: 0.0976 - val_loss: 2.3037 - val_accuracy: 0.0977 - 21s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "625/625 - 21s - loss: 2.3169 - accuracy: 0.0984 - val_loss: 2.3037 - val_accuracy: 0.0952 - 21s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "625/625 - 21s - loss: 2.3108 - accuracy: 0.0969 - val_loss: 2.3046 - val_accuracy: 0.0950 - 21s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "625/625 - 21s - loss: 2.3093 - accuracy: 0.1001 - val_loss: 2.3047 - val_accuracy: 0.0997 - 21s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "625/625 - 21s - loss: 2.3082 - accuracy: 0.1016 - val_loss: 2.2975 - val_accuracy: 0.1032 - 21s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "625/625 - 21s - loss: 2.2485 - accuracy: 0.1332 - val_loss: 2.2420 - val_accuracy: 0.1706 - 21s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "625/625 - 21s - loss: 2.1289 - accuracy: 0.1700 - val_loss: 2.1780 - val_accuracy: 0.1692 - 21s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "625/625 - 21s - loss: 2.1013 - accuracy: 0.1712 - val_loss: 2.0568 - val_accuracy: 0.1834 - 21s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "625/625 - 21s - loss: 2.0791 - accuracy: 0.1724 - val_loss: 2.1119 - val_accuracy: 0.1746 - 21s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "625/625 - 21s - loss: 2.0636 - accuracy: 0.1783 - val_loss: 2.0497 - val_accuracy: 0.1746 - 21s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "625/625 - 21s - loss: 2.0418 - accuracy: 0.1815 - val_loss: 2.0269 - val_accuracy: 0.1790 - 21s/epoch - 34ms/step\n",
      "Epoch 14/100\n",
      "625/625 - 21s - loss: 2.0338 - accuracy: 0.1841 - val_loss: 1.9919 - val_accuracy: 0.1956 - 21s/epoch - 34ms/step\n",
      "Epoch 15/100\n",
      "625/625 - 21s - loss: 2.0294 - accuracy: 0.1859 - val_loss: 2.0117 - val_accuracy: 0.1911 - 21s/epoch - 33ms/step\n",
      "Epoch 16/100\n",
      "625/625 - 16s - loss: 2.0255 - accuracy: 0.1903 - val_loss: 1.9813 - val_accuracy: 0.2056 - 16s/epoch - 25ms/step\n",
      "Epoch 17/100\n",
      "625/625 - 22s - loss: 2.0153 - accuracy: 0.1913 - val_loss: 2.0013 - val_accuracy: 0.2074 - 22s/epoch - 35ms/step\n",
      "Epoch 18/100\n",
      "625/625 - 21s - loss: 2.0092 - accuracy: 0.1944 - val_loss: 1.9674 - val_accuracy: 0.2193 - 21s/epoch - 34ms/step\n",
      "Epoch 19/100\n",
      "625/625 - 21s - loss: 2.0035 - accuracy: 0.2020 - val_loss: 1.9510 - val_accuracy: 0.2147 - 21s/epoch - 34ms/step\n",
      "Epoch 20/100\n",
      "625/625 - 21s - loss: 1.9939 - accuracy: 0.2071 - val_loss: 2.0812 - val_accuracy: 0.1973 - 21s/epoch - 34ms/step\n",
      "Epoch 21/100\n",
      "625/625 - 21s - loss: 1.9910 - accuracy: 0.2101 - val_loss: 2.0587 - val_accuracy: 0.2082 - 21s/epoch - 33ms/step\n",
      "Epoch 22/100\n",
      "625/625 - 21s - loss: 1.9873 - accuracy: 0.2182 - val_loss: 1.9243 - val_accuracy: 0.2361 - 21s/epoch - 33ms/step\n",
      "Epoch 23/100\n",
      "625/625 - 21s - loss: 1.9833 - accuracy: 0.2192 - val_loss: 1.9703 - val_accuracy: 0.2306 - 21s/epoch - 34ms/step\n",
      "Epoch 24/100\n",
      "625/625 - 21s - loss: 1.9800 - accuracy: 0.2239 - val_loss: 1.9211 - val_accuracy: 0.2493 - 21s/epoch - 33ms/step\n",
      "Epoch 25/100\n",
      "625/625 - 21s - loss: 1.9760 - accuracy: 0.2264 - val_loss: 1.9371 - val_accuracy: 0.2520 - 21s/epoch - 33ms/step\n",
      "Epoch 26/100\n",
      "625/625 - 21s - loss: 1.9723 - accuracy: 0.2349 - val_loss: 1.9542 - val_accuracy: 0.2460 - 21s/epoch - 33ms/step\n",
      "Epoch 27/100\n",
      "625/625 - 21s - loss: 1.9686 - accuracy: 0.2365 - val_loss: 2.2799 - val_accuracy: 0.1918 - 21s/epoch - 33ms/step\n",
      "Epoch 28/100\n",
      "625/625 - 21s - loss: 1.9616 - accuracy: 0.2438 - val_loss: 1.9195 - val_accuracy: 0.2606 - 21s/epoch - 33ms/step\n",
      "Epoch 29/100\n",
      "625/625 - 21s - loss: 1.9543 - accuracy: 0.2432 - val_loss: 1.9114 - val_accuracy: 0.2721 - 21s/epoch - 33ms/step\n",
      "Epoch 30/100\n",
      "625/625 - 21s - loss: 1.9563 - accuracy: 0.2466 - val_loss: 1.8755 - val_accuracy: 0.2849 - 21s/epoch - 33ms/step\n",
      "Epoch 31/100\n",
      "625/625 - 21s - loss: 1.9453 - accuracy: 0.2526 - val_loss: 1.8820 - val_accuracy: 0.2742 - 21s/epoch - 33ms/step\n",
      "Epoch 32/100\n",
      "625/625 - 21s - loss: 1.9368 - accuracy: 0.2520 - val_loss: 1.9403 - val_accuracy: 0.2629 - 21s/epoch - 34ms/step\n",
      "Epoch 33/100\n",
      "625/625 - 21s - loss: 1.9357 - accuracy: 0.2602 - val_loss: 1.8825 - val_accuracy: 0.2770 - 21s/epoch - 34ms/step\n",
      "Epoch 34/100\n",
      "625/625 - 21s - loss: 1.9292 - accuracy: 0.2576 - val_loss: 1.8601 - val_accuracy: 0.2812 - 21s/epoch - 34ms/step\n",
      "Epoch 35/100\n",
      "625/625 - 21s - loss: 1.9216 - accuracy: 0.2631 - val_loss: 1.8767 - val_accuracy: 0.2818 - 21s/epoch - 33ms/step\n",
      "Epoch 36/100\n",
      "625/625 - 21s - loss: 1.9139 - accuracy: 0.2646 - val_loss: 1.8950 - val_accuracy: 0.2793 - 21s/epoch - 33ms/step\n",
      "Epoch 37/100\n",
      "625/625 - 21s - loss: 1.9107 - accuracy: 0.2646 - val_loss: 1.8668 - val_accuracy: 0.2812 - 21s/epoch - 33ms/step\n",
      "Epoch 38/100\n",
      "625/625 - 21s - loss: 1.9076 - accuracy: 0.2684 - val_loss: 1.8471 - val_accuracy: 0.2927 - 21s/epoch - 33ms/step\n",
      "Epoch 39/100\n",
      "625/625 - 21s - loss: 1.9032 - accuracy: 0.2723 - val_loss: 1.8204 - val_accuracy: 0.2971 - 21s/epoch - 33ms/step\n",
      "Epoch 40/100\n",
      "625/625 - 21s - loss: 1.8931 - accuracy: 0.2711 - val_loss: 1.8138 - val_accuracy: 0.3001 - 21s/epoch - 33ms/step\n",
      "Epoch 41/100\n",
      "625/625 - 21s - loss: 1.8891 - accuracy: 0.2747 - val_loss: 1.8110 - val_accuracy: 0.2924 - 21s/epoch - 33ms/step\n",
      "Epoch 43/100\n",
      "625/625 - 21s - loss: 1.8820 - accuracy: 0.2762 - val_loss: 1.8164 - val_accuracy: 0.3008 - 21s/epoch - 33ms/step\n",
      "Epoch 44/100\n",
      "625/625 - 21s - loss: 1.8845 - accuracy: 0.2798 - val_loss: 1.8241 - val_accuracy: 0.2930 - 21s/epoch - 33ms/step\n",
      "Epoch 45/100\n",
      "625/625 - 21s - loss: 1.8774 - accuracy: 0.2831 - val_loss: 1.8150 - val_accuracy: 0.2935 - 21s/epoch - 34ms/step\n",
      "Epoch 46/100\n",
      "625/625 - 16s - loss: 1.8766 - accuracy: 0.2822 - val_loss: 1.8249 - val_accuracy: 0.3015 - 16s/epoch - 26ms/step\n",
      "Epoch 47/100\n",
      "625/625 - 21s - loss: 1.8719 - accuracy: 0.2817 - val_loss: 1.8450 - val_accuracy: 0.2929 - 21s/epoch - 33ms/step\n",
      "Epoch 48/100\n",
      "625/625 - 21s - loss: 1.8737 - accuracy: 0.2824 - val_loss: 1.8078 - val_accuracy: 0.3002 - 21s/epoch - 34ms/step\n",
      "Epoch 49/100\n",
      "625/625 - 21s - loss: 1.8693 - accuracy: 0.2830 - val_loss: 1.8445 - val_accuracy: 0.2814 - 21s/epoch - 34ms/step\n",
      "Epoch 50/100\n",
      "625/625 - 21s - loss: 1.8672 - accuracy: 0.2813 - val_loss: 1.8291 - val_accuracy: 0.2952 - 21s/epoch - 33ms/step\n",
      "Epoch 51/100\n",
      "625/625 - 21s - loss: 1.8687 - accuracy: 0.2847 - val_loss: 1.8223 - val_accuracy: 0.3058 - 21s/epoch - 34ms/step\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 1.7939 - accuracy: 0.3029\n",
      "Test accuracy: 0.3029\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, AlphaDropout, Dropout\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Function to create DNN model\n",
    "def create_dnn_model(hidden_layers, activation, dropout_rate, use_batch_norm=False):\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "        if use_batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        if activation == 'selu':\n",
    "            model.add(Activation('selu'))\n",
    "        else:\n",
    "            model.add(Activation('elu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define model parameters\n",
    "hidden_layers = 20\n",
    "activation = 'elu'\n",
    "dropout_rate = 0.5\n",
    "use_batch_norm = True\n",
    "\n",
    "# Create and compile the DNN model\n",
    "model = create_dnn_model(hidden_layers, activation, dropout_rate, use_batch_norm)\n",
    "optimizer = Nadam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
