{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1b2f4e",
   "metadata": {},
   "source": [
    "## CV_Assignment_3\n",
    "1. After each stride-2 conv, why do we double the number of filters?\n",
    "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
    "3. What data is saved by ActivationStats for each layer?\n",
    "4. How do we get a learner's callback after they've completed training?\n",
    "5. What are the drawbacks of activations above zero?\n",
    "6. Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "7. Why should we avoid starting training with a high learning rate?\n",
    "8. What are the pros of studying with a high rate of learning?\n",
    "9. Why do we want to end the training with a low learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 1:- Doubling the number of filters after each stride-2\n",
    "convolution helps the neural network capture increasingly complex and\n",
    "abstract features as the spatial dimensions decrease. This strategy\n",
    "ensures that the network can extract more diverse information\n",
    "despite downsampling. For example, if you start with 32 filters in\n",
    "one layer and double it to 64 in the next, it allows the\n",
    "network to learn richer representations as the receptive field\n",
    "grows.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 2:- Using a larger kernel (e.g., 5x5) in the first\n",
    "convolutional layer of an MNIST CNN helps capture more global and coarse\n",
    "features, as MNIST digits have relatively simple structures. This\n",
    "initial broader view aids in learning fundamental patterns.\n",
    "Subsequent layers can then focus on finer details with smaller\n",
    "kernels (e.g., 3x3) to build upon these basic features, leading to\n",
    "effective digit recognition.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 3:-  ActivationStats typically saves statistics related to\n",
    "activations in each layer during neural network training. This\n",
    "includes information such as mean and variance of activations,\n",
    "histograms, and other statistics that help monitor the behavior of\n",
    "neurons. These statistics are useful for diagnosing issues like\n",
    "vanishing gradients, exploding activations, or understanding how\n",
    "each layer transforms the input data during training, aiding in\n",
    "model optimization and debugging.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bcce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 4:- To get a learner's callback after training completion in a\n",
    "deep learning framework like Keras, you can use the\n",
    "on_train_end method. This method is part of the Keras callback API,\n",
    "allowing you to define custom actions to be performed after\n",
    "training. You can create a custom callback class by subclassing\n",
    "keras.callbacks.Callback and implementing the on_train_end method to specify the\n",
    "desired behavior upon training completion.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e42e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 5:-  Activations above zero in neural networks may lead to\n",
    "several drawbacks:  Vanishing Gradients: Positive activations can\n",
    "cause vanishing gradients during backpropagation, making\n",
    "training slower and more challenging.  Saturation: Neurons can\n",
    "saturate, making them less sensitive to changes in input. \n",
    "Overfitting: Excessive activations may contribute to overfitting as the\n",
    "network memorizes training data.  Exploding Gradients: In some\n",
    "cases, large positive activations can lead to exploding\n",
    "gradients, destabilizing training.  Balanced activations, such as\n",
    "ReLU or variants, are often used to mitigate these issues.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 6:- Benefits:\n",
    "\n",
    "1. Efficiency: Larger batches can utilize hardware more efficiently.\n",
    "2. Regularization: Noise in larger batches can act as a form of regularization,\n",
    "potentially improving generalization.\n",
    "\n",
    "Drawbacks:-  \n",
    "\n",
    "1. Slower Convergence: Larger batches may converge slower during training.\n",
    "2. Memory Usage: They require more memory, limiting model size or parallelism.\n",
    "3. Loss of Fine Details: Large batches may smooth out fine-grained information in data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 7:-  Starting training with a high learning rate can lead to\n",
    "issues such as divergent training, where the loss increases\n",
    "instead of decreasing. This happens because large learning rates\n",
    "cause overshooting the optimal weights. Gradually decreasing the\n",
    "learning rate during training (e.g., using learning rate schedules)\n",
    "allows for smoother convergence, avoids instability, and helps\n",
    "the model find a better minimum of the loss function.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 8:- Studying with a high learning rate in the context of\n",
    "neural networks, during the early training phase, can have some\n",
    "advantages:  Faster Convergence: High learning rates can lead to\n",
    "quicker initial convergence, saving training time. Escaping Local\n",
    "Minima: It helps escape shallow local minima and explore the loss\n",
    "landscape. Exploration: Higher learning rates encourage the model to\n",
    "explore a wider range of weight configurations. However, it's\n",
    "crucial to anneal or reduce the learning rate over time to ensure\n",
    "stable and accurate convergence.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffb35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 9:-  Ending training with a low learning rate is beneficial\n",
    "because it allows the model to fine-tune its weights in the\n",
    "vicinity of the global minimum. A low learning rate ensures that\n",
    "the optimization process proceeds slowly and steadily, helping\n",
    "the model converge to a more precise and stable solution. This\n",
    "helps prevent overshooting the optimal weights and facilitates\n",
    "better generalization on the validation and test datasets'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
