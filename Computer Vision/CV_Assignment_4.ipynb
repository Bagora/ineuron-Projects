{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615009d6",
   "metadata": {},
   "source": [
    "## CV_Assignment_4\n",
    "1. What is the concept of cyclical momentum?\n",
    "2. What callback keeps track of hyperparameter values (along with other data) during training?\n",
    "3. In the color dim plot, what does one column of pixels represent?\n",
    "4. In color dim, what does \"poor teaching\" look like? What is the reason for this?\n",
    "5. Does a batch normalization layer have any trainable parameters?\n",
    "6. In batch normalization during preparation, what statistics are used to normalize? What about during the validation process?\n",
    "7. Why do batch normalization layers help models generalize better?\n",
    "8. Explain between MAX POOLING and AVERAGE POOLING is number eight.\n",
    "9. What is the purpose of the POOLING LAYER?\n",
    "10. Why do we end up with Completely CONNECTED LAYERS?\n",
    "11. What do you mean by PARAMETERS?\n",
    "12. What formulas are used to measure these PARAMETERS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53455c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 1:- Cyclical momentum is a variation of the momentum\n",
    "optimization technique used in training neural networks. Unlike\n",
    "traditional momentum, which maintains a constant momentum coefficient\n",
    "throughout training, cyclical momentum dynamically changes the\n",
    "momentum coefficient during training in a cyclical pattern. This\n",
    "allows the model to explore a wider range of learning rates and\n",
    "momentums over time. Cyclical momentum can enhance optimization by\n",
    "adapting to the changing dynamics of the loss landscape,\n",
    "potentially leading to faster convergence and improved generalization.\n",
    "It's often used in conjunction with cyclical learning rates.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 2:- The TensorBoard callback is commonly used to keep track of\n",
    "hyperparameter values and other training-related data. It allows for\n",
    "real-time visualization and logging of various metrics such as loss,\n",
    "accuracy, and learning rates during training. This visualization and\n",
    "logging can help monitor the training process, fine-tune\n",
    "hyperparameters, and identify issues in the model, making it a valuable\n",
    "tool for deep learning practitioners.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 3:-  In a color dimension plot, one column of pixels\n",
    "represents the variation in color intensity or value for a specific\n",
    "pixel position across the entire image. Each pixel in the column\n",
    "corresponds to the same spatial position (e.g., the same x and y\n",
    "coordinates) within the image but varies in color based on the pixel's\n",
    "color channel values (e.g., Red, Green, Blue). This\n",
    "representation is a visual way to observe how color information changes\n",
    "vertically at a specific location in the image, offering insights\n",
    "into color gradients and patterns.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 4:- In a color dimension plot, \"poor teaching\" would manifest\n",
    "as a lack of diversity and contrast in the colors represented\n",
    "within a column of pixels. This means that the color values\n",
    "across the entire column are similar or nearly uniform,\n",
    "indicating a limited range of colors at that specific spatial\n",
    "position. The reason for this could be an insufficient variety of\n",
    "colors in the input data or ineffective training, which fails to\n",
    "capture the complexity and diversity of color patterns in the\n",
    "images, potentially hindering the model's ability to learn\n",
    "meaningful features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 5:- Yes, a batch normalization layer in a neural network has\n",
    "trainable parameters. It includes two learnable parameters per\n",
    "feature/channel: scale (gamma) and shift (beta). These parameters are\n",
    "learned during training to scale and shift the normalized\n",
    "activations, allowing the network to adapt and maintain the desired\n",
    "mean and variance for each feature, aiding in stable and\n",
    "efficient training.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a9c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=86528, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''Ans 6:- During training (preparation), batch normalization\n",
    "calculates the mean and variance of activations within the current\n",
    "mini-batch to normalize the data. This mini-batch statistics helps\n",
    "stabilize and speed up training. However, during the validation\n",
    "process, the network is usually tested on individual examples or a\n",
    "smaller batch without re-calculating batch statistics. Instead,\n",
    "the running population statistics (accumulated during\n",
    "training) are used for normalization. This ensures consistent\n",
    "normalization behavior during inference and prevents overfitting to the\n",
    "validation data.'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Batch size: 32, Channels: 64, Height: 28, Width: 28\n",
    "sample_data = torch.randn(32, 64, 28, 28)  \n",
    "\n",
    "# Define a neural network with batch normalization\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.fc = nn.Linear(128 * 26 * 26, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = Net()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dcdb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ans 7:- Batch normalization layers help models generalize\n",
    "better for several reasons:-\n",
    "\n",
    "1. Stabilizing Activation Distributions: By normalizing\n",
    "activations, batch normalization reduces internal covariate shift\n",
    "during training, making it easier for the model to learn and\n",
    "converge.\n",
    "\n",
    "2. Regularization Effect: The noise introduced by batch\n",
    "normalization during training acts as a form of regularization, reducing\n",
    "overfitting.\n",
    "\n",
    "3. Allowing Higher Learning Rates: It enables the use of\n",
    "higher learning rates, accelerating convergence without\n",
    "instability.\n",
    "\n",
    "4. Maintaining Consistent Statistics: During inference, batch\n",
    "normalization uses accumulated statistics, ensuring consistent behavior\n",
    "and improving generalization to unseen data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc86313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025622935AB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Original Input Data:\n",
      "[[  1.  28.   3.  45.]\n",
      " [ 15.   6.  77.   8.]\n",
      " [  7.  10.  55.  12.]\n",
      " [113. 914.  15. 186.]]\n",
      "\n",
      "Max Pooled Output:\n",
      "[[ 28.  77.]\n",
      " [914. 186.]]\n",
      "\n",
      "Average Pooled Output:\n",
      "[[ 12.5   33.25]\n",
      " [261.    67.  ]]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 8:- Max pooling and average pooling are two common pooling\n",
    "operations in convolutional neural networks. In max pooling, the\n",
    "maximum value within each pooling window is retained, highlighting\n",
    "the most significant feature. In average pooling, the average\n",
    "value is computed, giving a smoother representation. Max pooling\n",
    "is effective for detecting key features, while average\n",
    "pooling provides a more generalized representation, potentially\n",
    "preserving more spatial information. The choice depends on the\n",
    "specific task and desired trade-off between robustness and detail\n",
    "retention.'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "input_data = np.array([\n",
    "    [1, 28, 3, 45],\n",
    "    [15, 6, 77, 8],\n",
    "    [7, 10, 55, 12],\n",
    "    [113, 914, 15, 186]\n",
    "], dtype=np.float32)\n",
    "\n",
    "input_data = input_data.reshape((1, 4, 4, 1))\n",
    "\n",
    "# Create a TensorFlow model with max pooling and average pooling layers\n",
    "max_pooling_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4, 4, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "])\n",
    "\n",
    "average_pooling_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4, 4, 1)),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "])\n",
    "\n",
    "# Perform pooling\n",
    "max_pooled_data = max_pooling_model.predict(input_data)\n",
    "average_pooled_data = average_pooling_model.predict(input_data)\n",
    "\n",
    "print(\"Original Input Data:\")\n",
    "print(input_data.reshape(4, 4))\n",
    "print(\"\\nMax Pooled Output:\")\n",
    "print(max_pooled_data.reshape(2, 2))\n",
    "print(\"\\nAverage Pooled Output:\")\n",
    "print(average_pooled_data.reshape(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28e63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]]\n",
      "\n",
      "Max Pooled Output:\n",
      "[[ 6  8]\n",
      " [14 16]]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 9:- The pooling layer in a convolutional neural network serves\n",
    "several purposes:-\n",
    "\n",
    "1. Downsampling: It reduces the spatial dimensions of the\n",
    "feature maps, decreasing computational complexity.\n",
    "\n",
    "2. Feature Selection: By selecting the most important\n",
    "information within each pooling window (e.g., max pooling), it retains\n",
    "key features while discarding less relevant details.\n",
    "\n",
    "3. Translation Invariance: Pooling helps the network\n",
    "recognize features regardless of their precise location in the\n",
    "input.\n",
    "\n",
    "4. Reduction of Overfitting: Pooling can act as a form of\n",
    "regularization by reducing the model's sensitivity to noise and minor\n",
    "variations in the data.\n",
    "\n",
    "The output pooling with a 2x2 window and a stride of 2.\n",
    "Here's how the max pooling operation works on the input data:\n",
    "\n",
    "The input data is divided into non-overlapping 2x2 windows:-\n",
    "\n",
    "Window 1: [[1, 2], [5, 6]]\n",
    "Window 2: [[3, 4], [7, 8]]\n",
    "Window 3: [[9, 10], [13, 14]]\n",
    "Window 4: [[11, 12], [15, 16]]\n",
    "\n",
    "For each window, the maximum value is extracted\n",
    "\n",
    "Max of Window 1: 6\n",
    "Max of Window 2: 8\n",
    "Max of Window 3: 14\n",
    "Max of Window 4: 16\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample 2D input data (4x4)\n",
    "input_data = np.array([[1, 2, 3, 4],\n",
    "                      [5, 6, 7, 8],\n",
    "                      [9, 10, 11, 12],\n",
    "                      [13, 14, 15, 16]])\n",
    "\n",
    "# Define max pooling parameters (2x2 window, stride of 2)\n",
    "window_size = (2, 2)\n",
    "stride = 2\n",
    "\n",
    "# Perform max pooling\n",
    "output_data = []\n",
    "for i in range(0, input_data.shape[0], stride):\n",
    "    for j in range(0, input_data.shape[1], stride):\n",
    "        window = input_data[i:i+window_size[0], j:j+window_size[1]]\n",
    "        max_value = np.max(window)\n",
    "        output_data.append(max_value)\n",
    "\n",
    "# Reshape the output\n",
    "output_data = np.array(output_data).reshape((2, 2))\n",
    "\n",
    "print(\"Input Data:\")\n",
    "print(input_data)\n",
    "print(\"\\nMax Pooled Output:\")\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e569c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17226 (67.29 KB)\n",
      "Trainable params: 17226 (67.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''Ans 10:- Fully connected layers, also known as dense layers, are\n",
    "often used at the end of a neural network architecture to make\n",
    "predictions or perform classification tasks. They connect every neuron\n",
    "from the previous layer to each neuron in the current layer,\n",
    "allowing the model to learn complex relationships in the data.\n",
    "\n",
    "In this example, the Dense layers represent fully\n",
    "connected layers, and input_dim and output_dim specify the input and\n",
    "output dimensions, respectively. These layers enable the model to\n",
    "learn from the extracted features and make predictions for\n",
    "various tasks, such as image classification or regression.\n",
    "\n",
    "We import the necessary libraries from Keras. Create a\n",
    "sequential model using Sequential(). Add dense layers with different\n",
    "configurations, specifying the number of units and activation functions.\n",
    "Finally, we use model.summary() to print the architecture of the\n",
    "model, which includes the layer names, output shapes, and the\n",
    "number of trainable parameters.'''\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "input_dim = 64 \n",
    "output_dim = 10 \n",
    "\n",
    "# Add a dense layer with 128 units and ReLU activation function\n",
    "model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
    "\n",
    "# Add another dense layer with 64 units and ReLU activation function\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add a final dense layer with the desired output units and activation function\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1144dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5465]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5374], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "'''ANs 11:- In a neural network, parameters are the internal variables\n",
    "that the model learns from the training data. They include\n",
    "weights and biases associated with each neuron in the network.\n",
    "These parameters are adjusted during training to minimize the\n",
    "loss function, enabling the model to make accurate predictions.\n",
    "Here's a code example illustrating parameters in a simple linear\n",
    "regression model.\n",
    "\n",
    "In this example, model.parameters() returns a list of\n",
    "parameters, which includes the weight matrix and bias term of the\n",
    "linear regression model.'''\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# linear regression model\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "# Parameters (weights and bias) of the model\n",
    "params = list(model.parameters())\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051902db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.30929169058799744\n",
      "Epoch [200/1000], Loss: 0.2134174406528473\n",
      "Epoch [300/1000], Loss: 0.16733044385910034\n",
      "Epoch [400/1000], Loss: 0.13184557855129242\n",
      "Epoch [500/1000], Loss: 0.10433553904294968\n",
      "Epoch [600/1000], Loss: 0.08289802819490433\n",
      "Epoch [700/1000], Loss: 0.06610986590385437\n",
      "Epoch [800/1000], Loss: 0.052900925278663635\n",
      "Epoch [900/1000], Loss: 0.04246228560805321\n",
      "Epoch [1000/1000], Loss: 0.03417882323265076\n",
      "Trained Parameters:\n",
      "weight: [[1.6925193 2.4134233]]\n",
      "bias: [1.4666103]\n"
     ]
    }
   ],
   "source": [
    "'''Ans 12:- In a neural network, parameters are measured using\n",
    "various formulas during training. The primary formulas include:-\n",
    "\n",
    "1. Weight Update: Parameters are updated using gradient\n",
    "descent or its variants, where the new parameter value is computed\n",
    "by subtracting the gradient of the loss function with respect\n",
    "to the parameter.\n",
    "\n",
    "2. Bias Update: Similarly, bias terms are updated using gradient descent.\n",
    "\n",
    "In this code, the optimizer updates the parameters\n",
    "(model.parameters()) based on the gradients computed during the backpropagation\n",
    "step, which follows the loss function. The specific formulas\n",
    "used depend on the optimization algorithm employed.'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(0)\n",
    "input_data = torch.tensor(np.random.rand(100, 2), dtype=torch.float32)\n",
    "target_data = (2 * input_data[:, 0] + 3 * input_data[:, 1] + 1).view(-1, 1).detach()\n",
    "\n",
    "# Define a simple linear regression model\n",
    "model = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "# Define a loss function (mean squared error)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (stochastic gradient descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(input_data)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = loss_fn(predictions, target_data)\n",
    "    \n",
    "    # Backpropagation to compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss at each epoch\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Print the final trained parameters\n",
    "print('Trained Parameters:')\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
