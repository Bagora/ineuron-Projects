{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d864e8e9",
   "metadata": {},
   "source": [
    "## CV_Assignment_5\n",
    "1. How can each of these parameters be fine-tuned?\n",
    "    \n",
    "    • Number of hidden layers \n",
    "    \n",
    "    • Network architecture (network depth)\n",
    "\n",
    "    • Each layer's number of neurons (layer width)\n",
    "\n",
    "    • Form of activation\n",
    "\n",
    "    • Optimization and learning\n",
    "\n",
    "    • Learning rate and decay schedule\n",
    "\n",
    "    • Mini batch size\n",
    "\n",
    "    • Algorithms for optimization\n",
    "\n",
    "    • The number of epochs (and early stopping criteria)\n",
    "\n",
    "    • Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "    • L2 normalization\n",
    "\n",
    "    • Drop out layers\n",
    "    \n",
    "    • Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0735eb",
   "metadata": {},
   "source": [
    "Ans 1\n",
    "Fine-tuning neural network parameters:\n",
    "\n",
    "1. **Number of Hidden Layers**: Adjust the depth of the network by adding or removing layers.\n",
    "\n",
    "2. **Network Architecture (Depth)**: Modify the overall architecture, such as adding more convolutional or recurrent layers.\n",
    "\n",
    "3. **Layer Width**: Change the number of neurons in each layer to control capacity.\n",
    "\n",
    "4. **Activation Functions**: Experiment with different activation functions like ReLU, Sigmoid, or Tanh.\n",
    "\n",
    "5. **Optimization and Learning**: Use different optimizers like Adam, SGD, or RMSprop.\n",
    "\n",
    "6. **Learning Rate and Decay Schedule**: Tune the learning rate and apply learning rate schedules like step decay or exponential decay.\n",
    "\n",
    "7. **Mini Batch Size**: Adjust the size of mini-batches during training.\n",
    "\n",
    "8. **Optimization Algorithms**: Try alternative optimization algorithms, such as Nesterov momentum.\n",
    "\n",
    "9. **Number of Epochs**: Determine the suitable number of training epochs and implement early stopping.\n",
    "\n",
    "10. **Overfitting Prevention**: Use regularization techniques, including L2 normalization and dropout layers.\n",
    "\n",
    "11. **L2 Normalization**: Add L2 weight regularization to penalize large weights.\n",
    "\n",
    "12. **Dropout Layers**: Include dropout layers to prevent overfitting by randomly deactivating neurons.\n",
    "\n",
    "13. **Data Augmentation**: Augment training data with transformations like rotation, scaling, or cropping to increase dataset diversity.\n",
    "\n",
    "Each parameter adjustment impacts the model's performance, and fine-tuning involves finding the optimal combination for a specific task through experimentation and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d076827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 - 23s - loss: 2.3053 - accuracy: 0.1010 - val_loss: 2.3030 - val_accuracy: 0.0973 - 23s/epoch - 18ms/step\n",
      "Epoch 2/10\n",
      "1250/1250 - 21s - loss: 2.3027 - accuracy: 0.0986 - val_loss: 2.3028 - val_accuracy: 0.0933 - 21s/epoch - 17ms/step\n",
      "Epoch 3/10\n",
      "1250/1250 - 20s - loss: 2.3028 - accuracy: 0.0985 - val_loss: 2.3029 - val_accuracy: 0.0933 - 20s/epoch - 16ms/step\n",
      "Epoch 4/10\n",
      "1250/1250 - 21s - loss: 2.3028 - accuracy: 0.0993 - val_loss: 2.3029 - val_accuracy: 0.0933 - 21s/epoch - 17ms/step\n",
      "Epoch 5/10\n",
      "1250/1250 - 22s - loss: 2.3028 - accuracy: 0.0999 - val_loss: 2.3029 - val_accuracy: 0.0933 - 22s/epoch - 17ms/step\n",
      "Epoch 6/10\n",
      "1250/1250 - 21s - loss: 2.3029 - accuracy: 0.0994 - val_loss: 2.3029 - val_accuracy: 0.0933 - 21s/epoch - 17ms/step\n",
      "Epoch 7/10\n",
      "1250/1250 - 20s - loss: 2.3028 - accuracy: 0.0984 - val_loss: 2.3029 - val_accuracy: 0.0933 - 20s/epoch - 16ms/step\n",
      "Epoch 8/10\n",
      "1250/1250 - 21s - loss: 2.3027 - accuracy: 0.0998 - val_loss: 2.3029 - val_accuracy: 0.0979 - 21s/epoch - 17ms/step\n",
      "Epoch 9/10\n",
      "1250/1250 - 22s - loss: 2.3028 - accuracy: 0.0992 - val_loss: 2.3029 - val_accuracy: 0.0973 - 22s/epoch - 17ms/step\n",
      "Epoch 10/10\n",
      "1250/1250 - 21s - loss: 2.3028 - accuracy: 0.0997 - val_loss: 2.3029 - val_accuracy: 0.0933 - 21s/epoch - 17ms/step\n",
      "Fine-tuning Results:\n",
      "Training Accuracy: 0.10100000351667404\n",
      "Validation Accuracy: 0.09790000319480896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# base model\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Compile the model\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train) // batch_size,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2)\n",
    "\n",
    "# Fine-tuning results\n",
    "print(\"Fine-tuning Results:\")\n",
    "print(\"Training Accuracy:\", max(history.history['accuracy']))\n",
    "print(\"Validation Accuracy:\", max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1aa702",
   "metadata": {},
   "source": [
    "In this code, we perform the following fine-tuning steps:\n",
    "\n",
    "1. Define the neural network architecture.\n",
    "2. Set hyperparameters such as learning rate, batch size, epochs, and weight decay.\n",
    "3. Compile the model with an optimizer, loss function, and metrics.\n",
    "4. Apply data augmentation to increase dataset diversity.\n",
    "5. Train the model using data augmentation and monitor performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
