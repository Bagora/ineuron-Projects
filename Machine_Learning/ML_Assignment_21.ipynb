{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d7058f",
   "metadata": {},
   "source": [
    "## ML_Assignment_21\n",
    "1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?\n",
    "2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?\n",
    "3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?\n",
    "4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set?\n",
    "5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?\n",
    "6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "    a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "    b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "    c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "    d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "\n",
    "8. Follow these steps to grow a forest:\n",
    "\n",
    "      a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "\n",
    "      b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision        Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "     c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "     d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d19f62",
   "metadata": {},
   "source": [
    "### Ans 1\n",
    "\n",
    "The estimated depth of a Decision Tree trained on a one million instance training set can vary widely depending on several factors, including the complexity of the data, the features, and the hyperparameters chosen. However, we can provide a general estimate:\n",
    "\n",
    "1. **Complexity of Data**: If the data is relatively simple and can be well-separated by a few features, the Decision Tree may have a relatively shallow depth. In such cases, it might have a depth of fewer than 10 levels.\n",
    "\n",
    "2. **Complexity of Features**: If the dataset has a large number of features or if some features are highly informative, the tree might need more depth to capture the relationships between features and the target variable.\n",
    "\n",
    "3. **Hyperparameters**: The maximum depth of a Decision Tree can be controlled using hyperparameters like 'max_depth', 'min_samples_split', and 'min_samples_leaf'. If no depth restriction is applied (unrestricted), the tree could potentially become very deep, possibly deeper than necessary for good generalization.\n",
    "\n",
    "4. **Pruning**: Some Decision Tree algorithms perform pruning, which can reduce the depth of the tree after initial training to improve generalization.\n",
    "\n",
    "In practice, with a large and complex dataset like one million instances, an unrestricted Decision Tree might have a substantial depth, possibly ranging from dozens to hundreds of levels. However, it's essential to balance depth with the risk of overfitting, and hyperparameter tuning and pruning should be considered to optimize the tree's depth for better generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc397b",
   "metadata": {},
   "source": [
    "### Ans 2\n",
    "\n",
    "The Gini impurity of a node in a Decision Tree is typically lower than or equal to that of its parent. This is because Decision Trees aim to split data in a way that reduces impurity or disorder with each split. The Gini impurity measures the level of disorder or impurity in a node, and the goal is to create child nodes that are more homogenous than the parent node.\n",
    "\n",
    "While the Gini impurity is usually lower in child nodes, it's not an absolute rule. In some cases, the Gini impurity of a node may remain the same as its parent if the split does not improve homogeneity. Additionally, in very rare cases, the impurity might increase slightly due to random variations in data.\n",
    "\n",
    "However, as a general principle, Decision Trees are designed to decrease impurity when making splits, which often results in child nodes having lower Gini impurity than their parent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd41d6",
   "metadata": {},
   "source": [
    "### Ans 3\n",
    "\n",
    "Reducing the maximum depth of a Decision Tree can be an effective strategy to combat overfitting in the training set. Overfitting occurs when the tree is too complex, capturing noise and anomalies in the data rather than general patterns. Here's why reducing max depth is a good idea:\n",
    "\n",
    "1. **Simplicity**: Limiting the depth of the tree promotes a simpler model. A shallower tree is less likely to fit the training data's noise, making it more generalizable to unseen data.\n",
    "\n",
    "2. **Improved Generalization**: By reducing complexity, you increase the chances that the model will generalize well to new data. It becomes less specific to the training set and is more likely to capture underlying patterns.\n",
    "\n",
    "3. **Avoidance of Deep Branches**: Deep branches in a Decision Tree can become too specific to the training data, resulting in small subsets of data at the leaves. Reducing max depth prevents this issue and ensures that nodes have more instances, reducing overfitting.\n",
    "\n",
    "4. **Easier Interpretation**: A shallower tree is easier to interpret, making it more useful in explaining the decision-making process to stakeholders.\n",
    "\n",
    "However, it's essential to strike a balance. If you reduce the max depth too much, the model might underfit, failing to capture essential relationships in the data. Cross-validation and hyperparameter tuning can help you find the right depth that minimizes overfitting while maintaining good performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc4e81",
   "metadata": {},
   "source": [
    "### Ans 4\n",
    "\n",
    "Scaling the input features is generally not necessary and may not significantly impact the underfitting of a Decision Tree. Decision Trees are not sensitive to the scale of input features because they make decisions based on threshold values of individual features, not their magnitudes. Here's why scaling might not be a good solution for underfitting:\n",
    "\n",
    "1. **Feature Scaling**: Decision Trees split nodes based on feature values but not their magnitudes. Scaling won't affect the relative order or relationships between feature values.\n",
    "\n",
    "2. **Underfitting Causes**: Underfitting in Decision Trees typically results from excessive pruning or overly simplistic splits. Addressing underfitting usually involves strategies like increasing max depth, reducing min_samples_split, or using an ensemble of trees (Random Forests, Gradient Boosted Trees), rather than feature scaling.\n",
    "\n",
    "3. **Unwarranted Complexity**: Scaling can add unnecessary complexity to the preprocessing pipeline for Decision Trees and may not provide substantial benefits in terms of mitigating underfitting.\n",
    "\n",
    "Instead of scaling, focus on adjusting hyperparameters and tree depth to address underfitting and optimize the model's performance on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf4cf6",
   "metadata": {},
   "source": [
    "### Ans 5\n",
    "\n",
    "The time required to train a Decision Tree on a training set is influenced by factors like the number of instances, the complexity of the data, and the hardware/software environment. If it takes an hour to train a Decision Tree on a 1 million-instance dataset, we can estimate the time for a 10 million-instance dataset:\n",
    "\n",
    "Assuming training time scales linearly with the number of instances (which may not always be the case due to overhead and algorithmic considerations), we can estimate:\n",
    "\n",
    "Training time for 10 million instances â‰ˆ 10 hours\n",
    "\n",
    "This is a simplified estimate, and in practice, there might be non-linear scaling effects, resource limitations, and algorithmic optimizations that could make training time shorter or longer than this estimate. Additionally, hardware with more processing power or parallelization techniques could significantly reduce training time. Therefore, the actual time can vary, and it's advisable to conduct experiments to get a precise estimate for your specific dataset and setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e1cc5",
   "metadata": {},
   "source": [
    "### Ans 6\n",
    "\n",
    "Setting 'presort=True' in a Decision Tree algorithm can potentially speed up training for small datasets, but it is unlikely to have a significant impact on training time for a dataset with 100,000 instances. \n",
    "\n",
    "The 'presort' option causes the algorithm to pre-sort the data based on feature values before making splits. This can speed up training for smaller datasets because it reduces the time needed to find optimal splits during tree construction. However, sorting a large dataset can be computationally expensive and may actually slow down training for larger datasets, as the cost of sorting outweighs the benefits gained from presorting.\n",
    "\n",
    "In the case of a dataset with 100,000 instances, the decision to use presort would depend on the specific characteristics of the dataset and the available computational resources. It's advisable to experiment with both 'presort=True' and 'presort=False' settings to determine which one performs better for your dataset. For larger datasets, it's often recommended to leave presort as 'False'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa19a7d",
   "metadata": {},
   "source": [
    "### Ans 7\n",
    "\n",
    "Here's how we can train and fine-tune a Decision Tree for the moons dataset following the steps provided:-\n",
    "\n",
    "This code creates the moons dataset, performs a grid search for different values of max_leaf_nodes using cross-validation, selects the best hyperparameters, and then trains the Decision Tree classifier on the entire training set. Finally, it evaluates the model's accuracy on the test set, aiming for an accuracy in the range of 85% to 87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d95d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 87.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step a: Create the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# Step b: Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step c: Perform grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters from grid search\n",
    "best_max_leaf_nodes = grid_search.best_params_['max_leaf_nodes']\n",
    "\n",
    "# Step d: Train the model with best hyperparameters on the entire training set\n",
    "best_tree_clf = DecisionTreeClassifier(max_leaf_nodes=best_max_leaf_nodes, random_state=42)\n",
    "best_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = best_tree_clf.score(X_test, y_test)\n",
    "print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6ea1e",
   "metadata": {},
   "source": [
    "### Ans 8\n",
    "\n",
    "Here's how we can grow a Random Forest classifier following the steps provided:-\n",
    "\n",
    "In this code, we create 1,000 subsets of the training set, train Decision Trees on each subset, and then make predictions. The majority-vote predictions are used to evaluate the Random Forest's accuracy on the test set. we should achieve a slightly higher accuracy compared to the individual Decision Tree model, typically in the range of 0.5% to 1.5% higher. This demonstrates the power of ensemble methods like Random Forest in improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e167aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest on the test set: 87.60%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Step a: Create the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# Step b: Create and train 1,000 Decision Trees on subsets\n",
    "n_trees = 1000\n",
    "subset_size = 100\n",
    "tree_clfs = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X) - subset_size, random_state=42)\n",
    "for train_index, _ in rs.split(X):\n",
    "    tree_clf = DecisionTreeClassifier(max_leaf_nodes=best_max_leaf_nodes, random_state=42)\n",
    "    tree_clf.fit(X[train_index], y[train_index])\n",
    "    tree_clfs.append(tree_clf)\n",
    "\n",
    "# Step c: Make predictions and keep majority vote\n",
    "predictions = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "for i, tree_clf in enumerate(tree_clfs):\n",
    "    predictions[i] = tree_clf.predict(X_test)\n",
    "\n",
    "y_pred_majority_votes, _ = mode(predictions, axis=0)\n",
    "\n",
    "# Step d: Evaluate the Random Forest on the test set\n",
    "accuracy = np.mean(y_pred_majority_votes.ravel() == y_test)\n",
    "print(f\"Accuracy of Random Forest on the test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8cbefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
