{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb2ebd3",
   "metadata": {},
   "source": [
    "## ML_Assignment_8\n",
    "1. What exactly is a feature? Give an example to illustrate your point.\n",
    "2. What are the various circumstances in which feature construction is required?\n",
    "3. Describe how nominal variables are encoded.\n",
    "4. Describe how numeric features are converted to categorical features.\n",
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?\n",
    "6. When is a feature considered irrelevant? What can be said to quantify it?\n",
    "7. When is a function considered redundant? What criteria are used to identify features that could be redundant?\n",
    "8. What are the various distance measurements used to determine feature similarity?\n",
    "9. State difference between Euclidean and Manhattan distances?\n",
    "10. Distinguish between feature transformation and feature selection.\n",
    "11. Make brief notes on any two of the following:\n",
    "          \n",
    "      1. SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "      2. Collection of features using a hybrid approach\n",
    "\n",
    "      3. The width of the silhouette\n",
    "        \n",
    "      4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db3ec2",
   "metadata": {},
   "source": [
    "### Ans 1\n",
    "\n",
    "In machine learning and data analysis, a feature, also known as a variable or attribute, represents a characteristic or property of the data that is used to make predictions or gain insights. Features are the input variables that the model uses to learn patterns, relationships, and information from the data. They play a crucial role in determining a model's performance and its ability to make accurate predictions.\n",
    "\n",
    "Example:\n",
    "Let's consider a simple example of predicting house prices based on various features:\n",
    "\n",
    "In this housing price prediction example, 'SquareFeet,' 'Bedrooms,' 'Bathrooms,' 'Neighborhood,' 'YearBuilt,' and 'Garage' are the features used to predict the 'Price' of a house. These features provide valuable information that the model leverages to make accurate price predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc67eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SquareFeet</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Garage</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>1990</td>\n",
       "      <td>2</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1985</td>\n",
       "      <td>2</td>\n",
       "      <td>320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1800</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Rural</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SquareFeet  Bedrooms  Bathrooms Neighborhood  YearBuilt  Garage   Price\n",
       "0        1500         3          2     Suburban       1990       2  250000\n",
       "1        2000         4          3        Urban       1985       2  320000\n",
       "2        1800         3          2     Suburban       1995       2  280000\n",
       "3        2500         5          4        Rural       2000       3  380000\n",
       "4        1400         2          1        Urban       1975       1  200000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with house features\n",
    "data = {\n",
    "    'SquareFeet': [1500, 2000, 1800, 2500, 1400],\n",
    "    'Bedrooms': [3, 4, 3, 5, 2],\n",
    "    'Bathrooms': [2, 3, 2, 4, 1],\n",
    "    'Neighborhood': ['Suburban', 'Urban', 'Suburban', 'Rural', 'Urban'],\n",
    "    'YearBuilt': [1990, 1985, 1995, 2000, 1975],\n",
    "    'Garage': [2, 2, 2, 3, 1],\n",
    "    'Price': [250000, 320000, 280000, 380000, 200000]\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "# In this example, the features include 'SquareFeet', 'Bedrooms', 'Bathrooms', 'Neighborhood', 'YearBuilt', and 'Garage'.\n",
    "# 'Price' is the target variable we want to predict.\n",
    "\n",
    "# Features are used to build a predictive model that can estimate 'Price' based on these characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cfa24",
   "metadata": {},
   "source": [
    "### Ans 2\n",
    "\n",
    "Feature construction is required in various circumstances:\n",
    "1. **Dimensionality Reduction:** Reducing the number of features to improve model efficiency.\n",
    "2. **Non-linearity:** Transforming features to capture non-linear relationships in the data.\n",
    "3. **Encoding Categorical Data:** Converting categorical variables into numerical representations.\n",
    "4. **Feature Scaling:** Scaling features to have similar ranges for algorithms sensitive to scale.\n",
    "5. **Creating Interaction Terms:** Combining features to capture interactions.\n",
    "6. **Handling Missing Data:** Creating new features to represent missing data patterns.\n",
    "7. **Feature Extraction:** Reducing high-dimensional data using techniques like PCA or LDA.\n",
    "8. **Enhancing Interpretability:** Constructing features to make model outputs more interpretable or domain-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ddc8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "Polynomial Features:\n",
      " [[ 1.  1.  1.]\n",
      " [ 1.  2.  4.]\n",
      " [ 1.  3.  9.]\n",
      " [ 1.  4. 16.]\n",
      " [ 1.  5. 25.]]\n",
      "Scaled Features:\n",
      " [[ 0.         -1.41421356 -1.15624323]\n",
      " [ 0.         -0.70710678 -0.80937026]\n",
      " [ 0.          0.         -0.23124865]\n",
      " [ 0.          0.70710678  0.57812161]\n",
      " [ 0.          1.41421356  1.61874052]]\n"
     ]
    }
   ],
   "source": [
    "'''We start with a single feature (X) represented as an array\n",
    "[1, 2, 3, 4, 5]. We use PolynomialFeatures to perform feature\n",
    "construction, expanding it into polynomial features up to the second\n",
    "degree. Then, we use StandardScaler to scale the newly created\n",
    "features to have zero mean and unit variance. Feature construction\n",
    "allows us to capture non-linear relationships (in this case,\n",
    "quadratic) and scaling ensures that all features are on the same\n",
    "scale, which can be crucial for certain machine learning\n",
    "algorithms.'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Sample data with a single feature\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "\n",
    "# Feature construction\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Feature scaling (standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "print(\"Original Data:\\n\", X)\n",
    "print(\"Polynomial Features:\\n\", X_poly)\n",
    "print(\"Scaled Features:\\n\", X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7aa1e",
   "metadata": {},
   "source": [
    "### Ans 3\n",
    "\n",
    "Nominal variables, which represent categories or labels without any inherent order, are encoded using techniques like one-hot encoding. In one-hot encoding, each unique category is converted into a binary vector, with each category having its own binary column. A '1' is placed in the column corresponding to the category, while all other columns are filled with '0s'. This encoding preserves the distinction between categories but can increase dimensionality. It's commonly used in machine learning to handle nominal variables, making them suitable for algorithms that require numerical inputs.\n",
    "\n",
    "In this example, we have a nominal variable 'Color' with different categories (Red, Blue, Green). We use pd.get_dummies() to perform one-hot encoding, creating binary columns for each category. The resulting DataFrame (df_encoded) will have columns like 'Color_Red', 'Color_Blue', and 'Color_Green', where '1' indicates the presence of that color, and '0' indicates the absence. This is a common technique for encoding nominal variables in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960a7e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color_Blue</th>\n",
       "      <th>Color_Green</th>\n",
       "      <th>Color_Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Color_Blue  Color_Green  Color_Red\n",
       "0           0            0          1\n",
       "1           1            0          0\n",
       "2           0            1          0\n",
       "3           0            0          1\n",
       "4           1            0          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with a nominal variable 'Color'\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['Color'])\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f54a3b",
   "metadata": {},
   "source": [
    "### Ans 4\n",
    "\n",
    "Numeric features can be converted to categorical features by discretizing or binning the numerical values into distinct categories or intervals. This process is useful when you want to capture non-linear relationships or transform continuous data into a more interpretable format.\n",
    "\n",
    "In this example, the 'Age' feature is converted into the categorical 'Age_Category' feature by specifying age intervals (bins) and corresponding labels. This allows us to represent age groups rather than individual ages, making it easier to interpret and analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55987a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Age_Category\n",
       "0   25        Adult\n",
       "1   35        Adult\n",
       "2   42       Senior\n",
       "3   19        Adult\n",
       "4   56       Senior\n",
       "5   32        Adult\n",
       "6   48       Senior\n",
       "7   22        Adult"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with a numeric feature 'Age'\n",
    "data = {\n",
    "    'Age': [25, 35, 42, 19, 56, 32, 48, 22]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define age bins and labels\n",
    "age_bins = [0, 18, 35, 60]\n",
    "age_labels = ['Youth', 'Adult', 'Senior']\n",
    "\n",
    "# Convert 'Age' to categorical feature using cut()\n",
    "df['Age_Category'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels)\n",
    "df[['Age', 'Age_Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb89d99",
   "metadata": {},
   "source": [
    "### Ans 5\n",
    "\n",
    "The feature selection wrapper approach is a feature selection method that uses a machine learning model's performance as a criterion to select the most relevant subset of features from the original feature set. It involves repeatedly training and evaluating the model with different subsets of features to determine which combination yields the best performance. There are various techniques within the wrapper approach, such as forward selection, backward elimination, and recursive feature elimination (RFE).\n",
    "\n",
    "**Advantages of the Wrapper Approach:**\n",
    "1. **Optimizes for Model Performance:** The wrapper approach directly evaluates feature subsets based on their impact on the model's performance, which can result in selecting the most informative features for a specific task.\n",
    "\n",
    "2. **Handles Feature Interactions:** It can capture complex relationships and feature interactions that may not be apparent when considering individual features.\n",
    "\n",
    "3. **Customized to Model Choice:** It can be tailored to the specific machine learning algorithm being used, optimizing feature selection for that model.\n",
    "\n",
    "**Disadvantages of the Wrapper Approach:**\n",
    "1. **Computational Cost:** Repeatedly training and evaluating models with different feature subsets can be computationally expensive, especially for large datasets and complex models.\n",
    "\n",
    "2. **Risk of Overfitting:** There's a risk of overfitting the model to the training data when feature selection is based solely on model performance. It may result in selecting features that generalize poorly to new data.\n",
    "\n",
    "3. **May Not Scale Well:** The wrapper approach may not scale well to high-dimensional datasets with many features, as the number of possible feature combinations grows exponentially.\n",
    "\n",
    "4. **Dependent on Model Choice:** The effectiveness of the wrapper approach depends on the choice of the machine learning algorithm used for evaluation, which means different algorithms may yield different feature subsets.\n",
    "\n",
    "5. **Potential for Data Leakage:** When using the same data for both feature selection and model evaluation (without proper cross-validation), it can lead to data leakage and overly optimistic performance estimates.\n",
    "\n",
    "In summary, the wrapper approach is a powerful method for feature selection as it directly optimizes for model performance, but it comes with computational costs and the risk of overfitting. Careful consideration of the advantages and disadvantages is necessary when choosing this approach for feature selection in a specific machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c6555",
   "metadata": {},
   "source": [
    "### Ans 6\n",
    "\n",
    "A feature is considered irrelevant when it does not provide meaningful information for a specific machine learning task and does not contribute to improving the model's performance. Irrelevant features can introduce noise, increase dimensionality, and potentially degrade model accuracy. Quantifying the relevance of a feature often involves analyzing its impact on the model's performance or using statistical metrics.\n",
    "\n",
    "In this example, we compare the model's accuracy with and without an irrelevant feature (e.g., removing the first feature). If removing the feature has little to no impact on model accuracy, it can be considered irrelevant for the task. Quantifying feature relevance often involves experimentation and evaluation to assess their impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f7025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with irrelevant feature: 1.0\n",
      "Accuracy without irrelevant feature: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Add an irrelevant feature (all zeros)\n",
    "X_with_irrelevant_feature = np.hstack((X, np.zeros((X.shape[0], 1))))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_irrelevant_feature, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier (or any model of your choice)\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's accuracy on the test set with the irrelevant feature\n",
    "accuracy_with_irrelevant_feature = clf.score(X_test, y_test)\n",
    "\n",
    "# Remove the irrelevant feature (last column)\n",
    "X_train_removed_feature = X_train[:, :-1]\n",
    "X_test_removed_feature = X_test[:, :-1]\n",
    "clf_removed_feature = RandomForestClassifier(random_state=42)\n",
    "clf_removed_feature.fit(X_train_removed_feature, y_train)\n",
    "accuracy_without_irrelevant_feature = clf_removed_feature.score(X_test_removed_feature, y_test)\n",
    "\n",
    "print(\"Accuracy with irrelevant feature:\", accuracy_with_irrelevant_feature)\n",
    "print(\"Accuracy without irrelevant feature:\", accuracy_without_irrelevant_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc77b8",
   "metadata": {},
   "source": [
    "### Ans 7\n",
    "\n",
    "A function or feature is considered redundant when it doesn't provide additional or meaningful information to a model beyond what is already captured by other features. Identifying redundant features is essential to reduce dimensionality, enhance model interpretability, and avoid issues like multicollinearity. Several criteria and techniques are used to identify potentially redundant features:\n",
    "\n",
    "1. **Correlation:** Features with a high correlation coefficient (e.g., Pearson's correlation) may be redundant. High correlation implies that one feature can be predicted or explained by another.\n",
    "\n",
    "2. **Mutual Information:** Mutual information measures the dependence between two variables. If the mutual information between two features is close to zero, they may be redundant.\n",
    "\n",
    "3. **Feature Importance:** Techniques like Random Forest or Gradient Boosting can assess feature importance. Features with low importance scores may be candidates for redundancy.\n",
    "\n",
    "4. **Principal Component Analysis (PCA):** PCA can reveal linear combinations of features, and features that load heavily on the same principal component may be redundant.\n",
    "\n",
    "5. **Domain Knowledge:** Expert knowledge about the data and problem domain can help identify redundant features that don't contribute meaningfully to the task.\n",
    "\n",
    "6. **Sequential Feature Selection:** Algorithms like Sequential Backward Selection (SBS) can iteratively remove features that contribute the least to model performance.\n",
    "\n",
    "Identifying redundant features is a crucial step in feature selection and preprocessing to improve model efficiency and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d884ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features: {'Feature2'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with two correlated features\n",
    "np.random.seed(42)\n",
    "# Two random features\n",
    "data = np.random.rand(100, 2)  \n",
    "data[:, 1] = 2 * data[:, 0] + np.random.rand(100)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Feature1', 'Feature2'])\n",
    "\n",
    "# Calculate and visualize the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Identify and print features with a high correlation threshold\n",
    "high_corr_threshold = 0.8\n",
    "highly_correlated_features = set()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_features.add(colname)\n",
    "\n",
    "print(\"Highly correlated features:\", highly_correlated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53979226",
   "metadata": {},
   "source": [
    "### Ans 8\n",
    "\n",
    "Distance measurements are crucial for determining feature similarity or dissimilarity in various machine learning and data analysis tasks. Common distance metrics include:\n",
    "\n",
    "1. **Euclidean Distance:** The straight-line distance between two data points in Euclidean space. It is computed as the square root of the sum of squared differences between corresponding feature values.\n",
    "\n",
    "2. **Manhattan Distance:** The sum of absolute differences between corresponding feature values, often used in grid-based scenarios.\n",
    "\n",
    "3. **Cosine Similarity:** Measures the cosine of the angle between two feature vectors, indicating the similarity in direction. Often used for text and high-dimensional data.\n",
    "\n",
    "4. **Jaccard Similarity:** Used for sets, it measures the size of the intersection of two sets divided by the size of their union, commonly used in text analysis and recommendation systems.\n",
    "\n",
    "5. **Hamming Distance:** Measures the number of differing elements in binary data, often used in genetics and error correction.\n",
    "\n",
    "6. **Minkowski Distance:** A generalization of both Euclidean and Manhattan distances, with a parameter 'p' controlling the distance type.\n",
    "\n",
    "In this example, we compute the Euclidean distance and Cosine similarity between two feature vectors, 'vector1' and 'vector2,' using scikit-learn's distance calculation functions. These distance metrics are fundamental for various applications, such as clustering, recommendation systems, and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a0632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 5.196152422706632\n",
      "Cosine Similarity: 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "# Sample feature vectors\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 5, 6])\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "euclidean_dist = euclidean_distances(vector1.reshape(1, -1), vector2.reshape(1, -1))\n",
    "print(\"Euclidean Distance:\", euclidean_dist[0][0])\n",
    "\n",
    "# Calculate Cosine similarity (1 - Cosine distance)\n",
    "cosine_sim = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))\n",
    "print(\"Cosine Similarity:\", cosine_sim[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928123d",
   "metadata": {},
   "source": [
    "### Ans 9\n",
    "\n",
    "Euclidean and Manhattan distances are two commonly used distance metrics in machine learning and data analysis, and they differ in how they measure distance between data points:\n",
    "\n",
    "1. **Euclidean Distance:**\n",
    "   - Measures the straight-line (shortest) distance between two points in Euclidean space.\n",
    "   - Computed as the square root of the sum of squared differences between corresponding coordinates.\n",
    "   - Emphasizes the magnitude and direction of differences.\n",
    "   - Sensitive to variations in all dimensions.\n",
    "\n",
    "2. **Manhattan Distance:**\n",
    "   - Also known as L1 distance or taxicab distance.\n",
    "   - Measures the sum of absolute differences between corresponding coordinates.\n",
    "   - Only considers horizontal and vertical movements, like navigating city blocks.\n",
    "   - Less sensitive to outliers and variations in a single dimension.\n",
    "\n",
    "In this example, we calculate the Euclidean and Manhattan distances between two points, 'point1' and 'point2,' and observe how they differ in their measurement of distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33637104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 5.0\n",
      "Manhattan Distance: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "\n",
    "# Euclidean Distance\n",
    "euclidean_dist = distance.euclidean(point1, point2)\n",
    "\n",
    "# Manhattan Distance\n",
    "manhattan_dist = distance.cityblock(point1, point2)\n",
    "\n",
    "print(\"Euclidean Distance:\", euclidean_dist)\n",
    "print(\"Manhattan Distance:\", manhattan_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b5bf4",
   "metadata": {},
   "source": [
    "### Ans 10\n",
    "\n",
    "**Feature Transformation:** Feature transformation involves altering the representation of the original features to create new features. This can include techniques like scaling, normalization, or creating new features through mathematical operations like log transforms, polynomial features, or Principal Component Analysis (PCA). Feature transformation modifies the existing features and retains all or most of them in the dataset.\n",
    "\n",
    "**Feature Selection:** Feature selection, on the other hand, focuses on choosing a subset of the most relevant features from the original set. It aims to identify and keep the most informative features while discarding irrelevant or redundant ones. Feature selection techniques include filter methods (e.g., based on statistical tests), wrapper methods (e.g., using model performance), and embedded methods (e.g., algorithms that inherently select features). Feature selection reduces dimensionality and simplifies models.\n",
    "\n",
    "In this example, feature transformation involves standardization, while feature selection uses the chi-squared test to select the top 2 features from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155e7b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (150, 4)\n",
      "Transformed Data Shape: (150, 4)\n",
      "Selected Data Shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Feature Transformation: Standardization\n",
    "scaler = StandardScaler()\n",
    "X_transformed = scaler.fit_transform(X)\n",
    "\n",
    "# Feature Selection: Select the top 2 features using chi-squared test\n",
    "selector = SelectKBest(score_func=chi2, k=2)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Original Data Shape:\", X.shape)\n",
    "print(\"Transformed Data Shape:\", X_transformed.shape)\n",
    "print(\"Selected Data Shape:\", X_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2703e",
   "metadata": {},
   "source": [
    "### Ans 11\n",
    "\n",
    "1. **SVD (Singular Value Decomposition):** SVD is a matrix factorization technique used in dimensionality reduction and data compression. It decomposes a matrix into three matrices to identify underlying patterns. It's applied in recommendation systems, image compression, and text analysis.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   from scipy.linalg import svd\n",
    "\n",
    "   # Perform SVD on a matrix 'A'\n",
    "   U, S, VT = svd(A)\n",
    "   ```\n",
    "\n",
    "2. **Collection of Features Using a Hybrid Approach:** This refers to combining multiple feature selection methods to identify the most relevant features. It often involves a mix of filter, wrapper, and embedded techniques to improve feature selection results.\n",
    "\n",
    "3. **The Width of the Silhouette:** Silhouette width measures the separation between clusters in a clustering algorithm. A higher width indicates better-defined clusters. The width ranges from -1 to 1, with higher values indicating better clustering.\n",
    "\n",
    "4. **Receiver Operating Characteristic Curve (ROC Curve):** ROC curve is used to evaluate binary classification models. It plots the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity). AUC (Area Under the Curve) summarizes the ROC curve's performance.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "   fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "   auc = roc_auc_score(y_true, y_scores)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01492786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
